<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Swin Transformer解析 | Yin的笔记本</title>
    <meta name="generator" content="VuePress 1.5.0">
    <link rel="icon" href="/favicon.ico">
    <link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">
    <link rel="stylesheet" href="https://fastly.jsdelivr.net/github-markdown-css/2.2.1/github-markdown.css">
    <meta name="description" content="Yin的笔记本">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    <link rel="preload" href="/assets/css/0.styles.2d94763a.css" as="style"><link rel="preload" href="/assets/js/app.1c89d7eb.js" as="script"><link rel="preload" href="/assets/js/3.63b8c0eb.js" as="script"><link rel="preload" href="/assets/js/1.be3d2055.js" as="script"><link rel="preload" href="/assets/js/145.0826322f.js" as="script"><link rel="prefetch" href="/assets/js/10.7997888a.js"><link rel="prefetch" href="/assets/js/100.2278e289.js"><link rel="prefetch" href="/assets/js/101.12c66a26.js"><link rel="prefetch" href="/assets/js/102.6468aee3.js"><link rel="prefetch" href="/assets/js/103.d1dd7048.js"><link rel="prefetch" href="/assets/js/104.f4890013.js"><link rel="prefetch" href="/assets/js/105.a97deef5.js"><link rel="prefetch" href="/assets/js/106.c1bcc835.js"><link rel="prefetch" href="/assets/js/107.03ab02e9.js"><link rel="prefetch" href="/assets/js/108.ce9a9508.js"><link rel="prefetch" href="/assets/js/109.d03fd13e.js"><link rel="prefetch" href="/assets/js/11.f5bd6d01.js"><link rel="prefetch" href="/assets/js/110.13c311a4.js"><link rel="prefetch" href="/assets/js/111.6f30518f.js"><link rel="prefetch" href="/assets/js/112.ca951fb9.js"><link rel="prefetch" href="/assets/js/113.9c5c0582.js"><link rel="prefetch" href="/assets/js/114.eb24eb1a.js"><link rel="prefetch" href="/assets/js/115.de265fa2.js"><link rel="prefetch" href="/assets/js/116.24745fa4.js"><link rel="prefetch" href="/assets/js/117.86d4e841.js"><link rel="prefetch" href="/assets/js/118.9dbd6211.js"><link rel="prefetch" href="/assets/js/119.d8ec6ee3.js"><link rel="prefetch" href="/assets/js/12.770bcdd1.js"><link rel="prefetch" href="/assets/js/120.21b9b7e4.js"><link rel="prefetch" href="/assets/js/121.69c915c7.js"><link rel="prefetch" href="/assets/js/122.ddedddba.js"><link rel="prefetch" href="/assets/js/123.5a80347b.js"><link rel="prefetch" href="/assets/js/124.71bff2da.js"><link rel="prefetch" href="/assets/js/125.21830b23.js"><link rel="prefetch" href="/assets/js/126.f30185cf.js"><link rel="prefetch" href="/assets/js/127.3aeb1f53.js"><link rel="prefetch" href="/assets/js/128.6c296bf9.js"><link rel="prefetch" href="/assets/js/129.fda4528b.js"><link rel="prefetch" href="/assets/js/13.070330f5.js"><link rel="prefetch" href="/assets/js/130.9e6b67b0.js"><link rel="prefetch" href="/assets/js/131.20b0e822.js"><link rel="prefetch" href="/assets/js/132.a35815d1.js"><link rel="prefetch" href="/assets/js/133.bfa0daa0.js"><link rel="prefetch" href="/assets/js/134.eaea7aba.js"><link rel="prefetch" href="/assets/js/135.3a5c98a8.js"><link rel="prefetch" href="/assets/js/136.01223e0d.js"><link rel="prefetch" href="/assets/js/137.7cce857f.js"><link rel="prefetch" href="/assets/js/138.7a586d88.js"><link rel="prefetch" href="/assets/js/139.3f8891f1.js"><link rel="prefetch" href="/assets/js/14.760d4ae9.js"><link rel="prefetch" href="/assets/js/140.7d36bb0c.js"><link rel="prefetch" href="/assets/js/141.b2a3eaa6.js"><link rel="prefetch" href="/assets/js/142.8765938e.js"><link rel="prefetch" href="/assets/js/143.877037ca.js"><link rel="prefetch" href="/assets/js/144.0c63859f.js"><link rel="prefetch" href="/assets/js/146.0c7793ea.js"><link rel="prefetch" href="/assets/js/147.208c0c3d.js"><link rel="prefetch" href="/assets/js/148.e7e56ea0.js"><link rel="prefetch" href="/assets/js/149.269f672b.js"><link rel="prefetch" href="/assets/js/15.6dd745f8.js"><link rel="prefetch" href="/assets/js/150.5b335113.js"><link rel="prefetch" href="/assets/js/151.98e0c301.js"><link rel="prefetch" href="/assets/js/152.b8f48d96.js"><link rel="prefetch" href="/assets/js/153.bfeb3233.js"><link rel="prefetch" href="/assets/js/154.60508725.js"><link rel="prefetch" href="/assets/js/155.b8a1e762.js"><link rel="prefetch" href="/assets/js/156.a002dc45.js"><link rel="prefetch" href="/assets/js/157.5e547455.js"><link rel="prefetch" href="/assets/js/158.d2f2e77b.js"><link rel="prefetch" href="/assets/js/159.200ec9a4.js"><link rel="prefetch" href="/assets/js/16.6860f890.js"><link rel="prefetch" href="/assets/js/160.c842c71c.js"><link rel="prefetch" href="/assets/js/161.b69b41de.js"><link rel="prefetch" href="/assets/js/162.abaa0820.js"><link rel="prefetch" href="/assets/js/163.781d45a1.js"><link rel="prefetch" href="/assets/js/164.489c9172.js"><link rel="prefetch" href="/assets/js/165.4001e6a3.js"><link rel="prefetch" href="/assets/js/166.f605c3b7.js"><link rel="prefetch" href="/assets/js/167.187f7fef.js"><link rel="prefetch" href="/assets/js/168.8e751948.js"><link rel="prefetch" href="/assets/js/169.5c50b84f.js"><link rel="prefetch" href="/assets/js/17.96a61fae.js"><link rel="prefetch" href="/assets/js/170.c145b06c.js"><link rel="prefetch" href="/assets/js/171.1ff621e6.js"><link rel="prefetch" href="/assets/js/172.b5012bca.js"><link rel="prefetch" href="/assets/js/173.5bfa26a7.js"><link rel="prefetch" href="/assets/js/174.09fa2f55.js"><link rel="prefetch" href="/assets/js/175.d9227914.js"><link rel="prefetch" href="/assets/js/176.00adae9b.js"><link rel="prefetch" href="/assets/js/177.919b9f47.js"><link rel="prefetch" href="/assets/js/178.7e815bb2.js"><link rel="prefetch" href="/assets/js/179.7abd2d00.js"><link rel="prefetch" href="/assets/js/18.e9d7c71b.js"><link rel="prefetch" href="/assets/js/180.655d5d4a.js"><link rel="prefetch" href="/assets/js/181.b46b1de0.js"><link rel="prefetch" href="/assets/js/182.2e8f1a2a.js"><link rel="prefetch" href="/assets/js/183.22265de0.js"><link rel="prefetch" href="/assets/js/184.74507e51.js"><link rel="prefetch" href="/assets/js/185.bfa1144d.js"><link rel="prefetch" href="/assets/js/186.823452a1.js"><link rel="prefetch" href="/assets/js/187.4d75455f.js"><link rel="prefetch" href="/assets/js/188.916ab9d4.js"><link rel="prefetch" href="/assets/js/189.add9f2bf.js"><link rel="prefetch" href="/assets/js/19.f46a344f.js"><link rel="prefetch" href="/assets/js/190.193af7ee.js"><link rel="prefetch" href="/assets/js/191.30e19787.js"><link rel="prefetch" href="/assets/js/192.485b0912.js"><link rel="prefetch" href="/assets/js/193.ec31135d.js"><link rel="prefetch" href="/assets/js/194.b6f1ea36.js"><link rel="prefetch" href="/assets/js/195.f41818b6.js"><link rel="prefetch" href="/assets/js/196.3997fb55.js"><link rel="prefetch" href="/assets/js/197.9316466e.js"><link rel="prefetch" href="/assets/js/198.d6eaf8d5.js"><link rel="prefetch" href="/assets/js/199.7833a5cf.js"><link rel="prefetch" href="/assets/js/20.3e59b7db.js"><link rel="prefetch" href="/assets/js/200.09ac4b4c.js"><link rel="prefetch" href="/assets/js/201.c181d266.js"><link rel="prefetch" href="/assets/js/202.0c17f434.js"><link rel="prefetch" href="/assets/js/203.f325f13a.js"><link rel="prefetch" href="/assets/js/204.cc6ab889.js"><link rel="prefetch" href="/assets/js/205.5dea103e.js"><link rel="prefetch" href="/assets/js/206.d4bed4f0.js"><link rel="prefetch" href="/assets/js/207.570e9c30.js"><link rel="prefetch" href="/assets/js/208.15c57dde.js"><link rel="prefetch" href="/assets/js/209.052fb626.js"><link rel="prefetch" href="/assets/js/21.0d860ace.js"><link rel="prefetch" href="/assets/js/210.17da69e7.js"><link rel="prefetch" href="/assets/js/211.0437b47a.js"><link rel="prefetch" href="/assets/js/212.6d3516a5.js"><link rel="prefetch" href="/assets/js/213.2016ed1e.js"><link rel="prefetch" href="/assets/js/214.8d375069.js"><link rel="prefetch" href="/assets/js/215.6895df63.js"><link rel="prefetch" href="/assets/js/216.c044b620.js"><link rel="prefetch" href="/assets/js/217.1ff7a370.js"><link rel="prefetch" href="/assets/js/218.2b5c6610.js"><link rel="prefetch" href="/assets/js/219.3d1b7c68.js"><link rel="prefetch" href="/assets/js/22.395f0662.js"><link rel="prefetch" href="/assets/js/220.2f7d0eb4.js"><link rel="prefetch" href="/assets/js/221.0d5c483a.js"><link rel="prefetch" href="/assets/js/222.b4e8edd9.js"><link rel="prefetch" href="/assets/js/223.ed8cd7c5.js"><link rel="prefetch" href="/assets/js/224.70f8536d.js"><link rel="prefetch" href="/assets/js/225.877f00f4.js"><link rel="prefetch" href="/assets/js/226.e6433587.js"><link rel="prefetch" href="/assets/js/227.57bea06d.js"><link rel="prefetch" href="/assets/js/228.46f056e8.js"><link rel="prefetch" href="/assets/js/229.e075c130.js"><link rel="prefetch" href="/assets/js/23.918096ae.js"><link rel="prefetch" href="/assets/js/230.513baab1.js"><link rel="prefetch" href="/assets/js/231.107ef86e.js"><link rel="prefetch" href="/assets/js/232.498787e8.js"><link rel="prefetch" href="/assets/js/233.c9c37e3d.js"><link rel="prefetch" href="/assets/js/234.3205c614.js"><link rel="prefetch" href="/assets/js/235.6e73d096.js"><link rel="prefetch" href="/assets/js/236.1cd1beda.js"><link rel="prefetch" href="/assets/js/237.4708c31e.js"><link rel="prefetch" href="/assets/js/238.8e5fe2c7.js"><link rel="prefetch" href="/assets/js/239.d6960886.js"><link rel="prefetch" href="/assets/js/24.191c578f.js"><link rel="prefetch" href="/assets/js/240.ac10844d.js"><link rel="prefetch" href="/assets/js/241.d176cd12.js"><link rel="prefetch" href="/assets/js/242.05602258.js"><link rel="prefetch" href="/assets/js/243.04d4eea2.js"><link rel="prefetch" href="/assets/js/244.666552a4.js"><link rel="prefetch" href="/assets/js/245.391a0ebb.js"><link rel="prefetch" href="/assets/js/246.bbc73f23.js"><link rel="prefetch" href="/assets/js/247.2fb1308f.js"><link rel="prefetch" href="/assets/js/248.cb32facb.js"><link rel="prefetch" href="/assets/js/249.8f2c20c3.js"><link rel="prefetch" href="/assets/js/25.9fa7a0f1.js"><link rel="prefetch" href="/assets/js/250.aee82d40.js"><link rel="prefetch" href="/assets/js/251.b27f9949.js"><link rel="prefetch" href="/assets/js/252.0d3a1c08.js"><link rel="prefetch" href="/assets/js/253.dbac77db.js"><link rel="prefetch" href="/assets/js/254.daeb0894.js"><link rel="prefetch" href="/assets/js/255.9ae341b8.js"><link rel="prefetch" href="/assets/js/256.0914df92.js"><link rel="prefetch" href="/assets/js/257.17692151.js"><link rel="prefetch" href="/assets/js/258.6a51affa.js"><link rel="prefetch" href="/assets/js/259.cfa1b7e2.js"><link rel="prefetch" href="/assets/js/26.977b91ed.js"><link rel="prefetch" href="/assets/js/260.01a0a834.js"><link rel="prefetch" href="/assets/js/261.7c6786a8.js"><link rel="prefetch" href="/assets/js/262.40aa6ec8.js"><link rel="prefetch" href="/assets/js/263.9b768341.js"><link rel="prefetch" href="/assets/js/264.3516b6ee.js"><link rel="prefetch" href="/assets/js/265.24ebac38.js"><link rel="prefetch" href="/assets/js/266.d0f18960.js"><link rel="prefetch" href="/assets/js/267.cb67bfa7.js"><link rel="prefetch" href="/assets/js/268.49a739ea.js"><link rel="prefetch" href="/assets/js/269.319f9495.js"><link rel="prefetch" href="/assets/js/27.cbf02bc2.js"><link rel="prefetch" href="/assets/js/270.0ab89f8c.js"><link rel="prefetch" href="/assets/js/271.5ebfdaad.js"><link rel="prefetch" href="/assets/js/272.d978c09e.js"><link rel="prefetch" href="/assets/js/273.c2ca9559.js"><link rel="prefetch" href="/assets/js/274.ef3c0b4e.js"><link rel="prefetch" href="/assets/js/275.0cb280a9.js"><link rel="prefetch" href="/assets/js/276.f01bb24d.js"><link rel="prefetch" href="/assets/js/277.4336236b.js"><link rel="prefetch" href="/assets/js/278.91b80a10.js"><link rel="prefetch" href="/assets/js/279.a3dba48a.js"><link rel="prefetch" href="/assets/js/28.ea6f85eb.js"><link rel="prefetch" href="/assets/js/280.28c7e56d.js"><link rel="prefetch" href="/assets/js/281.b40bcb2f.js"><link rel="prefetch" href="/assets/js/282.8b95f9c1.js"><link rel="prefetch" href="/assets/js/283.112bae13.js"><link rel="prefetch" href="/assets/js/284.32679d92.js"><link rel="prefetch" href="/assets/js/285.5d940ba6.js"><link rel="prefetch" href="/assets/js/286.7644400d.js"><link rel="prefetch" href="/assets/js/287.3e55ea4a.js"><link rel="prefetch" href="/assets/js/288.e008719f.js"><link rel="prefetch" href="/assets/js/289.288c8559.js"><link rel="prefetch" href="/assets/js/29.09e2943e.js"><link rel="prefetch" href="/assets/js/290.5e886e4a.js"><link rel="prefetch" href="/assets/js/291.317a7f21.js"><link rel="prefetch" href="/assets/js/292.8e9221de.js"><link rel="prefetch" href="/assets/js/293.d2e72c3b.js"><link rel="prefetch" href="/assets/js/294.b08bb0a5.js"><link rel="prefetch" href="/assets/js/295.0ff3e8b8.js"><link rel="prefetch" href="/assets/js/296.8db2437f.js"><link rel="prefetch" href="/assets/js/297.df969d5b.js"><link rel="prefetch" href="/assets/js/298.5e04335e.js"><link rel="prefetch" href="/assets/js/299.30af4295.js"><link rel="prefetch" href="/assets/js/30.8ce7bb1c.js"><link rel="prefetch" href="/assets/js/300.bbee17de.js"><link rel="prefetch" href="/assets/js/301.658e30a9.js"><link rel="prefetch" href="/assets/js/302.83a48594.js"><link rel="prefetch" href="/assets/js/303.25b5c5d0.js"><link rel="prefetch" href="/assets/js/304.d1cda2a7.js"><link rel="prefetch" href="/assets/js/305.ec45274c.js"><link rel="prefetch" href="/assets/js/306.f5481c32.js"><link rel="prefetch" href="/assets/js/307.477c6cfe.js"><link rel="prefetch" href="/assets/js/308.2fc60e29.js"><link rel="prefetch" href="/assets/js/309.d3153724.js"><link rel="prefetch" href="/assets/js/31.9a171aa4.js"><link rel="prefetch" href="/assets/js/310.cd447685.js"><link rel="prefetch" href="/assets/js/311.aa55b174.js"><link rel="prefetch" href="/assets/js/312.975cb9f5.js"><link rel="prefetch" href="/assets/js/313.ff1ed35d.js"><link rel="prefetch" href="/assets/js/314.0f909559.js"><link rel="prefetch" href="/assets/js/315.db32db01.js"><link rel="prefetch" href="/assets/js/32.947f503d.js"><link rel="prefetch" href="/assets/js/33.59e6f3ca.js"><link rel="prefetch" href="/assets/js/34.d611cc6b.js"><link rel="prefetch" href="/assets/js/35.1bcf17ab.js"><link rel="prefetch" href="/assets/js/36.4ce4f82a.js"><link rel="prefetch" href="/assets/js/37.0054f8c4.js"><link rel="prefetch" href="/assets/js/38.b4bdef00.js"><link rel="prefetch" href="/assets/js/39.436a4cf2.js"><link rel="prefetch" href="/assets/js/4.71970d79.js"><link rel="prefetch" href="/assets/js/40.3c0f9211.js"><link rel="prefetch" href="/assets/js/41.db84a37b.js"><link rel="prefetch" href="/assets/js/42.d490b527.js"><link rel="prefetch" href="/assets/js/43.53ff8034.js"><link rel="prefetch" href="/assets/js/44.6ac3003e.js"><link rel="prefetch" href="/assets/js/45.c5e657d2.js"><link rel="prefetch" href="/assets/js/46.9c37bbc9.js"><link rel="prefetch" href="/assets/js/47.8cc0235f.js"><link rel="prefetch" href="/assets/js/48.46917f1a.js"><link rel="prefetch" href="/assets/js/49.4956c196.js"><link rel="prefetch" href="/assets/js/5.b5c477bb.js"><link rel="prefetch" href="/assets/js/50.c350ef1e.js"><link rel="prefetch" href="/assets/js/51.228f9fb4.js"><link rel="prefetch" href="/assets/js/52.d0ea8325.js"><link rel="prefetch" href="/assets/js/53.bcf8531d.js"><link rel="prefetch" href="/assets/js/54.823cdd86.js"><link rel="prefetch" href="/assets/js/55.dd739e97.js"><link rel="prefetch" href="/assets/js/56.0a6fa41e.js"><link rel="prefetch" href="/assets/js/57.e8c7ac20.js"><link rel="prefetch" href="/assets/js/58.9a6ccc24.js"><link rel="prefetch" href="/assets/js/59.a3e64ba9.js"><link rel="prefetch" href="/assets/js/6.8d75c7c7.js"><link rel="prefetch" href="/assets/js/60.4ef57d04.js"><link rel="prefetch" href="/assets/js/61.b2221af0.js"><link rel="prefetch" href="/assets/js/62.d5e651a6.js"><link rel="prefetch" href="/assets/js/63.7cf7bbf5.js"><link rel="prefetch" href="/assets/js/64.a1b2ab5f.js"><link rel="prefetch" href="/assets/js/65.795f8ce1.js"><link rel="prefetch" href="/assets/js/66.766bae3c.js"><link rel="prefetch" href="/assets/js/67.33fffa58.js"><link rel="prefetch" href="/assets/js/68.34cad213.js"><link rel="prefetch" href="/assets/js/69.44ab1539.js"><link rel="prefetch" href="/assets/js/7.c826a528.js"><link rel="prefetch" href="/assets/js/70.0b157f44.js"><link rel="prefetch" href="/assets/js/71.dda3257f.js"><link rel="prefetch" href="/assets/js/72.e4fd59e4.js"><link rel="prefetch" href="/assets/js/73.5bfac475.js"><link rel="prefetch" href="/assets/js/74.106dce7c.js"><link rel="prefetch" href="/assets/js/75.febe432a.js"><link rel="prefetch" href="/assets/js/76.9d7b680c.js"><link rel="prefetch" href="/assets/js/77.53d02bdd.js"><link rel="prefetch" href="/assets/js/78.78f8bd3e.js"><link rel="prefetch" href="/assets/js/79.765020db.js"><link rel="prefetch" href="/assets/js/8.94c271b3.js"><link rel="prefetch" href="/assets/js/80.58708562.js"><link rel="prefetch" href="/assets/js/81.b496be82.js"><link rel="prefetch" href="/assets/js/82.e7fb031b.js"><link rel="prefetch" href="/assets/js/83.9a97b3c4.js"><link rel="prefetch" href="/assets/js/84.46388e53.js"><link rel="prefetch" href="/assets/js/85.273a246a.js"><link rel="prefetch" href="/assets/js/86.0f1a102d.js"><link rel="prefetch" href="/assets/js/87.0340887c.js"><link rel="prefetch" href="/assets/js/88.2464434e.js"><link rel="prefetch" href="/assets/js/89.9c7259c6.js"><link rel="prefetch" href="/assets/js/9.13823324.js"><link rel="prefetch" href="/assets/js/90.fc961b8c.js"><link rel="prefetch" href="/assets/js/91.05ce823e.js"><link rel="prefetch" href="/assets/js/92.6f598c32.js"><link rel="prefetch" href="/assets/js/93.9b4a4e9b.js"><link rel="prefetch" href="/assets/js/94.e181fada.js"><link rel="prefetch" href="/assets/js/95.1f019ce8.js"><link rel="prefetch" href="/assets/js/96.c5fdf710.js"><link rel="prefetch" href="/assets/js/97.64f5f96a.js"><link rel="prefetch" href="/assets/js/98.e761ab9f.js"><link rel="prefetch" href="/assets/js/99.b19b68c9.js">
    <link rel="stylesheet" href="/assets/css/0.styles.2d94763a.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container" data-v-42dd5e05><div data-v-42dd5e05><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-42dd5e05 data-v-42dd5e05><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-95c9a4e8 data-v-42dd5e05 data-v-42dd5e05><h3 class="title" style="display:none;" data-v-95c9a4e8 data-v-95c9a4e8>Yin的笔记本</h3> <!----> <label id="box" class="inputBox" style="display:none;" data-v-95c9a4e8 data-v-95c9a4e8><input type="password" value="" data-v-95c9a4e8> <span data-v-95c9a4e8>Konck! Knock!</span> <button data-v-95c9a4e8>OK</button></label> <div class="footer" style="display:none;" data-v-95c9a4e8 data-v-95c9a4e8><span data-v-95c9a4e8><i class="iconfont reco-theme" data-v-95c9a4e8></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-95c9a4e8>vuePress-theme-reco</a></span> <span data-v-95c9a4e8><i class="iconfont reco-copyright" data-v-95c9a4e8></i> <a data-v-95c9a4e8><span data-v-95c9a4e8>Howard Yin</span>
            
          <span data-v-95c9a4e8>2021 - </span>
          2025
        </a></span></div></div> <div class="hide" data-v-42dd5e05><header class="navbar" data-v-42dd5e05><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/logo.png" alt="Yin的笔记本" class="logo"> <span class="site-name">Yin的笔记本</span></a> <div class="links"><div class="color-picker"><a class="color-button"><i class="iconfont reco-color"></i></a> <div class="color-picker-menu" style="display:none;"><div class="mode-options"><h4 class="title">Choose mode</h4> <ul class="color-mode-options"><li class="dark">dark</li><li class="auto active">auto</li><li class="light">light</li></ul></div></div></div> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/CNCF/" class="nav-link"><i class="iconfont undefined"></i>
  CNCF
</a></li><li class="dropdown-item"><!----> <a href="/categories/Docker/" class="nav-link"><i class="iconfont undefined"></i>
  Docker
</a></li><li class="dropdown-item"><!----> <a href="/categories/namespaces/" class="nav-link"><i class="iconfont undefined"></i>
  namespaces
</a></li><li class="dropdown-item"><!----> <a href="/categories/Kubernetes/" class="nav-link"><i class="iconfont undefined"></i>
  Kubernetes
</a></li><li class="dropdown-item"><!----> <a href="/categories/Kubernetes对象/" class="nav-link"><i class="iconfont undefined"></i>
  Kubernetes对象
</a></li><li class="dropdown-item"><!----> <a href="/categories/Linux/" class="nav-link"><i class="iconfont undefined"></i>
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/categories/MyIdeas/" class="nav-link"><i class="iconfont undefined"></i>
  MyIdeas
</a></li><li class="dropdown-item"><!----> <a href="/categories/Revolution/" class="nav-link"><i class="iconfont undefined"></i>
  Revolution
</a></li><li class="dropdown-item"><!----> <a href="/categories/WebRTC/" class="nav-link"><i class="iconfont undefined"></i>
  WebRTC
</a></li><li class="dropdown-item"><!----> <a href="/categories/云计算/" class="nav-link"><i class="iconfont undefined"></i>
  云计算
</a></li><li class="dropdown-item"><!----> <a href="/categories/人工智能/" class="nav-link"><i class="iconfont undefined"></i>
  人工智能
</a></li><li class="dropdown-item"><!----> <a href="/categories/分布式/" class="nav-link"><i class="iconfont undefined"></i>
  分布式
</a></li><li class="dropdown-item"><!----> <a href="/categories/图像处理/" class="nav-link"><i class="iconfont undefined"></i>
  图像处理
</a></li><li class="dropdown-item"><!----> <a href="/categories/图形学/" class="nav-link"><i class="iconfont undefined"></i>
  图形学
</a></li><li class="dropdown-item"><!----> <a href="/categories/微服务/" class="nav-link"><i class="iconfont undefined"></i>
  微服务
</a></li><li class="dropdown-item"><!----> <a href="/categories/数学/" class="nav-link"><i class="iconfont undefined"></i>
  数学
</a></li><li class="dropdown-item"><!----> <a href="/categories/OJ笔记/" class="nav-link"><i class="iconfont undefined"></i>
  OJ笔记
</a></li><li class="dropdown-item"><!----> <a href="/categories/博弈论/" class="nav-link"><i class="iconfont undefined"></i>
  博弈论
</a></li><li class="dropdown-item"><!----> <a href="/categories/形式语言与自动机/" class="nav-link"><i class="iconfont undefined"></i>
  形式语言与自动机
</a></li><li class="dropdown-item"><!----> <a href="/categories/数据库/" class="nav-link"><i class="iconfont undefined"></i>
  数据库
</a></li><li class="dropdown-item"><!----> <a href="/categories/服务器运维/" class="nav-link"><i class="iconfont undefined"></i>
  服务器运维
</a></li><li class="dropdown-item"><!----> <a href="/categories/编程语言/" class="nav-link"><i class="iconfont undefined"></i>
  编程语言
</a></li><li class="dropdown-item"><!----> <a href="/categories/C/" class="nav-link"><i class="iconfont undefined"></i>
  C
</a></li><li class="dropdown-item"><!----> <a href="/categories/Git/" class="nav-link"><i class="iconfont undefined"></i>
  Git
</a></li><li class="dropdown-item"><!----> <a href="/categories/Go/" class="nav-link"><i class="iconfont undefined"></i>
  Go
</a></li><li class="dropdown-item"><!----> <a href="/categories/Java/" class="nav-link"><i class="iconfont undefined"></i>
  Java
</a></li><li class="dropdown-item"><!----> <a href="/categories/JavaScript/" class="nav-link"><i class="iconfont undefined"></i>
  JavaScript
</a></li><li class="dropdown-item"><!----> <a href="/categories/Python/" class="nav-link"><i class="iconfont undefined"></i>
  Python
</a></li><li class="dropdown-item"><!----> <a href="/categories/Nvidia/" class="nav-link"><i class="iconfont undefined"></i>
  Nvidia
</a></li><li class="dropdown-item"><!----> <a href="/categories/Shell/" class="nav-link"><i class="iconfont undefined"></i>
  Shell
</a></li><li class="dropdown-item"><!----> <a href="/categories/Tex/" class="nav-link"><i class="iconfont undefined"></i>
  Tex
</a></li><li class="dropdown-item"><!----> <a href="/categories/Rust/" class="nav-link"><i class="iconfont undefined"></i>
  Rust
</a></li><li class="dropdown-item"><!----> <a href="/categories/Vue/" class="nav-link"><i class="iconfont undefined"></i>
  Vue
</a></li><li class="dropdown-item"><!----> <a href="/categories/视频编解码/" class="nav-link"><i class="iconfont undefined"></i>
  视频编解码
</a></li><li class="dropdown-item"><!----> <a href="/categories/计算机网络/" class="nav-link"><i class="iconfont undefined"></i>
  计算机网络
</a></li><li class="dropdown-item"><!----> <a href="/categories/SDN/" class="nav-link"><i class="iconfont undefined"></i>
  SDN
</a></li><li class="dropdown-item"><!----> <a href="/categories/论文笔记/" class="nav-link"><i class="iconfont undefined"></i>
  论文笔记
</a></li><li class="dropdown-item"><!----> <a href="/categories/讨论/" class="nav-link"><i class="iconfont undefined"></i>
  讨论
</a></li><li class="dropdown-item"><!----> <a href="/categories/边缘计算/" class="nav-link"><i class="iconfont undefined"></i>
  边缘计算
</a></li><li class="dropdown-item"><!----> <a href="/categories/量子信息技术/" class="nav-link"><i class="iconfont undefined"></i>
  量子信息技术
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><a href="http://profile.yindaheng98.top" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-account"></i>
  About
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <a href="https://github.com/yindaheng98/Notebook/tree/master/学习笔记" target="_blank" rel="noopener noreferrer" class="repo-link"><i class="iconfont reco-查看源码"></i>
    查看源码
    <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></nav></div></header> <div class="sidebar-mask" data-v-42dd5e05></div> <aside class="sidebar" data-v-42dd5e05><div class="personal-info-wrapper" data-v-0ac41e2e><img src="/avatar.svg" alt="author-avatar" class="personal-img" data-v-0ac41e2e> <h3 class="name" data-v-0ac41e2e>
    Howard Yin
  </h3> <div class="num" data-v-0ac41e2e><div data-v-0ac41e2e><h3 data-v-0ac41e2e>304</h3> <h6 data-v-0ac41e2e>Article</h6></div> <div data-v-0ac41e2e><h3 data-v-0ac41e2e>153</h3> <h6 data-v-0ac41e2e>Tag</h6></div></div> <hr data-v-0ac41e2e></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/CNCF/" class="nav-link"><i class="iconfont undefined"></i>
  CNCF
</a></li><li class="dropdown-item"><!----> <a href="/categories/Docker/" class="nav-link"><i class="iconfont undefined"></i>
  Docker
</a></li><li class="dropdown-item"><!----> <a href="/categories/namespaces/" class="nav-link"><i class="iconfont undefined"></i>
  namespaces
</a></li><li class="dropdown-item"><!----> <a href="/categories/Kubernetes/" class="nav-link"><i class="iconfont undefined"></i>
  Kubernetes
</a></li><li class="dropdown-item"><!----> <a href="/categories/Kubernetes对象/" class="nav-link"><i class="iconfont undefined"></i>
  Kubernetes对象
</a></li><li class="dropdown-item"><!----> <a href="/categories/Linux/" class="nav-link"><i class="iconfont undefined"></i>
  Linux
</a></li><li class="dropdown-item"><!----> <a href="/categories/MyIdeas/" class="nav-link"><i class="iconfont undefined"></i>
  MyIdeas
</a></li><li class="dropdown-item"><!----> <a href="/categories/Revolution/" class="nav-link"><i class="iconfont undefined"></i>
  Revolution
</a></li><li class="dropdown-item"><!----> <a href="/categories/WebRTC/" class="nav-link"><i class="iconfont undefined"></i>
  WebRTC
</a></li><li class="dropdown-item"><!----> <a href="/categories/云计算/" class="nav-link"><i class="iconfont undefined"></i>
  云计算
</a></li><li class="dropdown-item"><!----> <a href="/categories/人工智能/" class="nav-link"><i class="iconfont undefined"></i>
  人工智能
</a></li><li class="dropdown-item"><!----> <a href="/categories/分布式/" class="nav-link"><i class="iconfont undefined"></i>
  分布式
</a></li><li class="dropdown-item"><!----> <a href="/categories/图像处理/" class="nav-link"><i class="iconfont undefined"></i>
  图像处理
</a></li><li class="dropdown-item"><!----> <a href="/categories/图形学/" class="nav-link"><i class="iconfont undefined"></i>
  图形学
</a></li><li class="dropdown-item"><!----> <a href="/categories/微服务/" class="nav-link"><i class="iconfont undefined"></i>
  微服务
</a></li><li class="dropdown-item"><!----> <a href="/categories/数学/" class="nav-link"><i class="iconfont undefined"></i>
  数学
</a></li><li class="dropdown-item"><!----> <a href="/categories/OJ笔记/" class="nav-link"><i class="iconfont undefined"></i>
  OJ笔记
</a></li><li class="dropdown-item"><!----> <a href="/categories/博弈论/" class="nav-link"><i class="iconfont undefined"></i>
  博弈论
</a></li><li class="dropdown-item"><!----> <a href="/categories/形式语言与自动机/" class="nav-link"><i class="iconfont undefined"></i>
  形式语言与自动机
</a></li><li class="dropdown-item"><!----> <a href="/categories/数据库/" class="nav-link"><i class="iconfont undefined"></i>
  数据库
</a></li><li class="dropdown-item"><!----> <a href="/categories/服务器运维/" class="nav-link"><i class="iconfont undefined"></i>
  服务器运维
</a></li><li class="dropdown-item"><!----> <a href="/categories/编程语言/" class="nav-link"><i class="iconfont undefined"></i>
  编程语言
</a></li><li class="dropdown-item"><!----> <a href="/categories/C/" class="nav-link"><i class="iconfont undefined"></i>
  C
</a></li><li class="dropdown-item"><!----> <a href="/categories/Git/" class="nav-link"><i class="iconfont undefined"></i>
  Git
</a></li><li class="dropdown-item"><!----> <a href="/categories/Go/" class="nav-link"><i class="iconfont undefined"></i>
  Go
</a></li><li class="dropdown-item"><!----> <a href="/categories/Java/" class="nav-link"><i class="iconfont undefined"></i>
  Java
</a></li><li class="dropdown-item"><!----> <a href="/categories/JavaScript/" class="nav-link"><i class="iconfont undefined"></i>
  JavaScript
</a></li><li class="dropdown-item"><!----> <a href="/categories/Python/" class="nav-link"><i class="iconfont undefined"></i>
  Python
</a></li><li class="dropdown-item"><!----> <a href="/categories/Nvidia/" class="nav-link"><i class="iconfont undefined"></i>
  Nvidia
</a></li><li class="dropdown-item"><!----> <a href="/categories/Shell/" class="nav-link"><i class="iconfont undefined"></i>
  Shell
</a></li><li class="dropdown-item"><!----> <a href="/categories/Tex/" class="nav-link"><i class="iconfont undefined"></i>
  Tex
</a></li><li class="dropdown-item"><!----> <a href="/categories/Rust/" class="nav-link"><i class="iconfont undefined"></i>
  Rust
</a></li><li class="dropdown-item"><!----> <a href="/categories/Vue/" class="nav-link"><i class="iconfont undefined"></i>
  Vue
</a></li><li class="dropdown-item"><!----> <a href="/categories/视频编解码/" class="nav-link"><i class="iconfont undefined"></i>
  视频编解码
</a></li><li class="dropdown-item"><!----> <a href="/categories/计算机网络/" class="nav-link"><i class="iconfont undefined"></i>
  计算机网络
</a></li><li class="dropdown-item"><!----> <a href="/categories/SDN/" class="nav-link"><i class="iconfont undefined"></i>
  SDN
</a></li><li class="dropdown-item"><!----> <a href="/categories/论文笔记/" class="nav-link"><i class="iconfont undefined"></i>
  论文笔记
</a></li><li class="dropdown-item"><!----> <a href="/categories/讨论/" class="nav-link"><i class="iconfont undefined"></i>
  讨论
</a></li><li class="dropdown-item"><!----> <a href="/categories/边缘计算/" class="nav-link"><i class="iconfont undefined"></i>
  边缘计算
</a></li><li class="dropdown-item"><!----> <a href="/categories/量子信息技术/" class="nav-link"><i class="iconfont undefined"></i>
  量子信息技术
</a></li></ul></div></div><div class="nav-item"><a href="/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><a href="http://profile.yindaheng98.top" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-account"></i>
  About
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <a href="https://github.com/yindaheng98/Notebook/tree/master/学习笔记" target="_blank" rel="noopener noreferrer" class="repo-link"><i class="iconfont reco-查看源码"></i>
    查看源码
    <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>Swin Transformer解析</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#最上层" class="sidebar-link">最上层</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#类型和初始化" class="sidebar-link">类型和初始化</a></li><li class="sidebar-sub-header"><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#推断过程" class="sidebar-link">推断过程</a></li></ul></li><li><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#patch-embedding-patchembed" class="sidebar-link">Patch Embedding PatchEmbed</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#主体模块-basiclayer" class="sidebar-link">主体模块 BasicLayer</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#patch-merging-patchmerging" class="sidebar-link">Patch Merging PatchMerging</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#初始化" class="sidebar-link">初始化</a></li><li class="sidebar-sub-header"><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#推断过程-2" class="sidebar-link">推断过程</a></li></ul></li><li><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#主体模块-swintransformerblock" class="sidebar-link">主体模块 SwinTransformerBlock</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#window-partition-reverse" class="sidebar-link">Window Partition/Reverse</a></li><li class="sidebar-sub-header"><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#初始化-2" class="sidebar-link">初始化</a></li><li class="sidebar-sub-header"><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#推断过程-3" class="sidebar-link">推断过程</a></li><li class="sidebar-sub-header"><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#总结" class="sidebar-link">总结</a></li></ul></li><li><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#核心模块windowattention" class="sidebar-link">核心模块WindowAttention</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#初始化-3" class="sidebar-link">初始化</a></li><li class="sidebar-sub-header"><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#推断过程-4" class="sidebar-link">推断过程</a></li><li class="sidebar-sub-header"><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#总结-2" class="sidebar-link">总结</a></li></ul></li><li><a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/SwinTransformer.html#总结-3" class="sidebar-link">总结</a><ul class="sidebar-sub-headers"></ul></li></ul></section></li></ul> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-95c9a4e8 data-v-42dd5e05><h3 class="title" style="display:none;" data-v-95c9a4e8 data-v-95c9a4e8>Swin Transformer解析</h3> <!----> <label id="box" class="inputBox" style="display:none;" data-v-95c9a4e8 data-v-95c9a4e8><input type="password" value="" data-v-95c9a4e8> <span data-v-95c9a4e8>Konck! Knock!</span> <button data-v-95c9a4e8>OK</button></label> <div class="footer" style="display:none;" data-v-95c9a4e8 data-v-95c9a4e8><span data-v-95c9a4e8><i class="iconfont reco-theme" data-v-95c9a4e8></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-95c9a4e8>vuePress-theme-reco</a></span> <span data-v-95c9a4e8><i class="iconfont reco-copyright" data-v-95c9a4e8></i> <a data-v-95c9a4e8><span data-v-95c9a4e8>Howard Yin</span>
            
          <span data-v-95c9a4e8>2021 - </span>
          2025
        </a></span></div></div> <div data-v-42dd5e05><main class="page"><div class="page-title" style="display:none;"><h1>Swin Transformer解析</h1> <hr> <div data-v-3d9cec6e><i class="iconfont reco-account" data-v-3d9cec6e><span data-v-3d9cec6e>Howard Yin</span></i> <i class="iconfont reco-date" data-v-3d9cec6e><span data-v-3d9cec6e>2022-06-12 12:22:02</span></i> <!----> <i class="iconfont reco-tag tags" data-v-3d9cec6e><span class="tag-item" data-v-3d9cec6e>人工智能</span><span class="tag-item" data-v-3d9cec6e>Transformer</span><span class="tag-item" data-v-3d9cec6e>SwinTransformer</span><span class="tag-item" data-v-3d9cec6e>神经网络</span><span class="tag-item" data-v-3d9cec6e>注意力机制</span></i></div></div> <div class="theme-reco-content content__default" style="display:none;"><h2 id="最上层"><a href="#最上层" class="header-anchor">#</a> 最上层</h2> <p>先看看最上层：</p> <h3 id="类型和初始化"><a href="#类型和初始化" class="header-anchor">#</a> 类型和初始化</h3> <p>初始化的部分有很详细的参数注释，不多解释。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">SwinTransformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r&quot;&quot;&quot; Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030
    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img_size<span class="token operator">=</span><span class="token number">224</span><span class="token punctuation">,</span> patch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> in_chans<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>
                 embed_dim<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">,</span> depths<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                 window_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> mlp_ratio<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> qkv_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> qk_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 drop_rate<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> attn_drop_rate<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> drop_path_rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
                 norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">,</span> ape<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> patch_norm<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                 use_checkpoint<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>num_classes <span class="token operator">=</span> num_classes
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>depths<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> embed_dim
        self<span class="token punctuation">.</span>ape <span class="token operator">=</span> ape
        self<span class="token punctuation">.</span>patch_norm <span class="token operator">=</span> patch_norm
        self<span class="token punctuation">.</span>num_features <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>embed_dim <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">**</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>mlp_ratio <span class="token operator">=</span> mlp_ratio
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br></div></div><p>初始化了 Patch Embedding 这个<code>PatchEmbed</code>函数应该就是 Patch Embedding 算法，后面重点介绍</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token comment"># split image into non-overlapping patches</span>
        self<span class="token punctuation">.</span>patch_embed <span class="token operator">=</span> PatchEmbed<span class="token punctuation">(</span>
            img_size<span class="token operator">=</span>img_size<span class="token punctuation">,</span> patch_size<span class="token operator">=</span>patch_size<span class="token punctuation">,</span> in_chans<span class="token operator">=</span>in_chans<span class="token punctuation">,</span> embed_dim<span class="token operator">=</span>embed_dim<span class="token punctuation">,</span>
            norm_layer<span class="token operator">=</span>norm_layer <span class="token keyword">if</span> self<span class="token punctuation">.</span>patch_norm <span class="token keyword">else</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
        num_patches <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">.</span>num_patches
        patches_resolution <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">.</span>patches_resolution
        self<span class="token punctuation">.</span>patches_resolution <span class="token operator">=</span> patches_resolution

        <span class="token comment"># absolute position embedding</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>ape<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>absolute_pos_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> num_patches<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
            trunc_normal_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>absolute_pos_embed<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">.02</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>用了Dropout</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        self<span class="token punctuation">.</span>pos_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>drop_rate<span class="token punctuation">)</span>

        <span class="token comment"># stochastic depth</span>
        dpr <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> drop_path_rate<span class="token punctuation">,</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>depths<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token comment"># stochastic depth decay rule</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>初始化模型主干，<code>BasicLayer</code>函数就是 Swin Transformer 主体模块，后面重点介绍</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token comment"># build layers</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i_layer <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            layer <span class="token operator">=</span> BasicLayer<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>embed_dim <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">**</span> i_layer<span class="token punctuation">)</span><span class="token punctuation">,</span>
                               input_resolution<span class="token operator">=</span><span class="token punctuation">(</span>patches_resolution<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">**</span> i_layer<span class="token punctuation">)</span><span class="token punctuation">,</span>
                                                 patches_resolution<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">**</span> i_layer<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                               depth<span class="token operator">=</span>depths<span class="token punctuation">[</span>i_layer<span class="token punctuation">]</span><span class="token punctuation">,</span>
                               num_heads<span class="token operator">=</span>num_heads<span class="token punctuation">[</span>i_layer<span class="token punctuation">]</span><span class="token punctuation">,</span>
                               window_size<span class="token operator">=</span>window_size<span class="token punctuation">,</span>
                               mlp_ratio<span class="token operator">=</span>self<span class="token punctuation">.</span>mlp_ratio<span class="token punctuation">,</span>
                               qkv_bias<span class="token operator">=</span>qkv_bias<span class="token punctuation">,</span> qk_scale<span class="token operator">=</span>qk_scale<span class="token punctuation">,</span>
                               drop<span class="token operator">=</span>drop_rate<span class="token punctuation">,</span> attn_drop<span class="token operator">=</span>attn_drop_rate<span class="token punctuation">,</span>
                               drop_path<span class="token operator">=</span>dpr<span class="token punctuation">[</span><span class="token builtin">sum</span><span class="token punctuation">(</span>depths<span class="token punctuation">[</span><span class="token punctuation">:</span>i_layer<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token builtin">sum</span><span class="token punctuation">(</span>depths<span class="token punctuation">[</span><span class="token punctuation">:</span>i_layer <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                               norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">,</span>
                               downsample<span class="token operator">=</span>PatchMerging <span class="token keyword">if</span> <span class="token punctuation">(</span>i_layer <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>num_layers <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                               use_checkpoint<span class="token operator">=</span>use_checkpoint<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>接下来是LayerNorm和线性Head，不多介绍；最后还有个初始化权重的，也不必多说</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_features<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>avgpool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool1d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_features<span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span> <span class="token keyword">if</span> num_classes <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">else</span> nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_init_weights<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_init_weights</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>
            trunc_normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">.02</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span> <span class="token keyword">and</span> m<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">)</span><span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><h3 id="推断过程"><a href="#推断过程" class="header-anchor">#</a> 推断过程</h3> <ol><li>Patch Embedding</li> <li>Dropout</li> <li>Swin Transformer 主干</li> <li>LayerNorm</li> <li>平均值池化</li> <li>Head</li></ol> <div class="language-python line-numbers-mode"><pre class="language-python"><code>    <span class="token keyword">def</span> <span class="token function">forward_features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>ape<span class="token punctuation">:</span>
            x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>absolute_pos_embed
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># B L C</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>avgpool<span class="token punctuation">(</span>x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># B C 1</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>forward_features<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>head<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p><img src="i/SwinTransformer.png" alt=""></p> <h2 id="patch-embedding-patchembed"><a href="#patch-embedding-patchembed" class="header-anchor">#</a> Patch Embedding <code>PatchEmbed</code></h2> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">PatchEmbed</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r&quot;&quot;&quot; Image to Patch Embedding
    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> img_size<span class="token operator">=</span><span class="token number">224</span><span class="token punctuation">,</span> patch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> in_chans<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> embed_dim<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        img_size <span class="token operator">=</span> to_2tuple<span class="token punctuation">(</span>img_size<span class="token punctuation">)</span>
        patch_size <span class="token operator">=</span> to_2tuple<span class="token punctuation">(</span>patch_size<span class="token punctuation">)</span>
        patches_resolution <span class="token operator">=</span> <span class="token punctuation">[</span>img_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> patch_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> img_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> patch_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>img_size <span class="token operator">=</span> img_size
        self<span class="token punctuation">.</span>patch_size <span class="token operator">=</span> patch_size
        self<span class="token punctuation">.</span>patches_resolution <span class="token operator">=</span> patches_resolution
        self<span class="token punctuation">.</span>num_patches <span class="token operator">=</span> patches_resolution<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> patches_resolution<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

        self<span class="token punctuation">.</span>in_chans <span class="token operator">=</span> in_chans
        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> embed_dim
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>Patch Embedding实际上就是一个卷积，输入<code>patch_size</code>x<code>patch_size</code>x<code>in_chans</code>的Patch，输出1x1x<code>embed_dim</code>，然后把<code>stride</code>设成和<code>patch_size</code>一样大，于是就是对不重叠的Patch进行操作。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_chans<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>patch_size<span class="token punctuation">,</span> stride<span class="token operator">=</span>patch_size<span class="token punctuation">)</span>
        <span class="token keyword">if</span> norm_layer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>embed_dim<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>norm <span class="token operator">=</span> <span class="token boolean">None</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>计算过程也没啥好讲的，执行Patch Embedding的那个卷积之后结果显然是<code>img_size/patch_size</code>x<code>img_size/patch_size</code>x<code>embed_dim</code>的矩阵，用<code>flatten</code>展平成<code>img_size^2/patch_size^2</code>x<code>embed_dim</code>矩阵即可。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        B<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        <span class="token comment"># FIXME look at relaxing size constraints</span>
        <span class="token keyword">assert</span> H <span class="token operator">==</span> self<span class="token punctuation">.</span>img_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">and</span> W <span class="token operator">==</span> self<span class="token punctuation">.</span>img_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> \
            <span class="token string-interpolation"><span class="token string">f&quot;Input image size (</span><span class="token interpolation"><span class="token punctuation">{</span>H<span class="token punctuation">}</span></span><span class="token string">*</span><span class="token interpolation"><span class="token punctuation">{</span>W<span class="token punctuation">}</span></span><span class="token string">) doesn't match model (</span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>img_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">*</span><span class="token interpolation"><span class="token punctuation">{</span>self<span class="token punctuation">.</span>img_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">).&quot;</span></span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># B Ph*Pw C</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>norm <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h2 id="主体模块-basiclayer"><a href="#主体模块-basiclayer" class="header-anchor">#</a> 主体模块 <code>BasicLayer</code></h2> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">BasicLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot; A basic Swin Transformer layer for one stage.
    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> input_resolution<span class="token punctuation">,</span> depth<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span>
                 mlp_ratio<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> qkv_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> qk_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> drop<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> attn_drop<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>
                 drop_path<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">,</span> downsample<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> use_checkpoint<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
        self<span class="token punctuation">.</span>input_resolution <span class="token operator">=</span> input_resolution
        self<span class="token punctuation">.</span>depth <span class="token operator">=</span> depth
        self<span class="token punctuation">.</span>use_checkpoint <span class="token operator">=</span> use_checkpoint
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br></div></div><p>又套了一个<code>SwinTransformerBlock</code>还是没有触及核心，后面重点解析这个<code>SwinTransformerBlock</code>。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token comment"># build blocks</span>
        self<span class="token punctuation">.</span>blocks <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>
            SwinTransformerBlock<span class="token punctuation">(</span>dim<span class="token operator">=</span>dim<span class="token punctuation">,</span> input_resolution<span class="token operator">=</span>input_resolution<span class="token punctuation">,</span>
                                 num_heads<span class="token operator">=</span>num_heads<span class="token punctuation">,</span> window_size<span class="token operator">=</span>window_size<span class="token punctuation">,</span>
                                 shift_size<span class="token operator">=</span><span class="token number">0</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">else</span> window_size <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span>
                                 mlp_ratio<span class="token operator">=</span>mlp_ratio<span class="token punctuation">,</span>
                                 qkv_bias<span class="token operator">=</span>qkv_bias<span class="token punctuation">,</span> qk_scale<span class="token operator">=</span>qk_scale<span class="token punctuation">,</span>
                                 drop<span class="token operator">=</span>drop<span class="token punctuation">,</span> attn_drop<span class="token operator">=</span>attn_drop<span class="token punctuation">,</span>
                                 drop_path<span class="token operator">=</span>drop_path<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>drop_path<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span> <span class="token keyword">else</span> drop_path<span class="token punctuation">,</span>
                                 norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>depth<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><p>这个<code>downsample</code>是外面来的函数，从最上层的调用看，传进来的是一个<code>PatchMerging</code>函数，后面也得解析一下。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token comment"># patch merging layer</span>
        <span class="token keyword">if</span> downsample <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>downsample <span class="token operator">=</span> downsample<span class="token punctuation">(</span>input_resolution<span class="token punctuation">,</span> dim<span class="token operator">=</span>dim<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>norm_layer<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>downsample <span class="token operator">=</span> <span class="token boolean">None</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>推断过程就是先<code>SwinTransformerBlock</code>再<code>downsample</code>。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> blk <span class="token keyword">in</span> self<span class="token punctuation">.</span>blocks<span class="token punctuation">:</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_checkpoint<span class="token punctuation">:</span>
                x <span class="token operator">=</span> checkpoint<span class="token punctuation">.</span>checkpoint<span class="token punctuation">(</span>blk<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                x <span class="token operator">=</span> blk<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>downsample <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>downsample<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h2 id="patch-merging-patchmerging"><a href="#patch-merging-patchmerging" class="header-anchor">#</a> Patch Merging <code>PatchMerging</code></h2> <p>这<code>PatchMerging</code>是Swin Transformer的核心创新点之一，虽然很短但很重要。</p> <h3 id="初始化"><a href="#初始化" class="header-anchor">#</a> 初始化</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">PatchMerging</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r&quot;&quot;&quot; Patch Merging Layer.
    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_resolution<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>input_resolution <span class="token operator">=</span> input_resolution
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>这是LayerNorm和线性映射层，后面的Patch Merging操作会将4个Patch叠在一起，所以这里的输入是4x<code>dim</code>；然后按照论文讲的，Patch Merging将4个Patch叠在一起后再用线性层将通道数减半，所以这里的输出是2x<code>dim</code></p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        self<span class="token punctuation">.</span>reduction <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> dim<span class="token punctuation">,</span> <span class="token number">2</span> <span class="token operator">*</span> dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> dim<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><img src="i/SwinTransformerLayer.png" alt=""></p> <p>有个注意点是到这里Patch已经是一个一维向量了，而这里的<code>nn.Linear</code>只与输入的<code>dim</code>有关而与Patch数量无关，根据调用链，这个<code>dim=int(embed_dim * 2 ** i_layer)</code>，<code>embed_dim</code>是一维Patch Embedding的长度，输出的<code>i_layer</code>是第几层。看上图可以知道，特征图channel数每过一个Swin Transformer层就翻倍，所以这个<code>dim</code>实际上也只和Patch Embedding的长度有关，这意味着每个Patch都是经过相同的线性计算。由于图像大小固定，Patch数量也是固定的，这里实际上可以给每个Patch单用一个线性层参数，但作者没有这么用。</p> <h3 id="推断过程-2"><a href="#推断过程-2" class="header-anchor">#</a> 推断过程</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        x: B, H*W, C
        &quot;&quot;&quot;</span>
        H<span class="token punctuation">,</span> W <span class="token operator">=</span> self<span class="token punctuation">.</span>input_resolution
        B<span class="token punctuation">,</span> L<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        <span class="token keyword">assert</span> L <span class="token operator">==</span> H <span class="token operator">*</span> W<span class="token punctuation">,</span> <span class="token string">&quot;input feature has wrong size&quot;</span>
        <span class="token keyword">assert</span> H <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> W <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f&quot;x size (</span><span class="token interpolation"><span class="token punctuation">{</span>H<span class="token punctuation">}</span></span><span class="token string">*</span><span class="token interpolation"><span class="token punctuation">{</span>W<span class="token punctuation">}</span></span><span class="token string">) are not even.&quot;</span></span>

        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>最核心的就是下面这个，这个操作就是把相邻的2x2共4个Patch叠在一起，然后用线性层将通道数减半。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        x0 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># B H/2 W/2 C</span>
        x1 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># B H/2 W/2 C</span>
        x2 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># B H/2 W/2 C</span>
        x3 <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># B H/2 W/2 C</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x0<span class="token punctuation">,</span> x1<span class="token punctuation">,</span> x2<span class="token punctuation">,</span> x3<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># B H/2 W/2 4*C</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span> <span class="token operator">*</span> C<span class="token punctuation">)</span>  <span class="token comment"># B H/2*W/2 4*C</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>reduction<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><h2 id="主体模块-swintransformerblock"><a href="#主体模块-swintransformerblock" class="header-anchor">#</a> 主体模块 <code>SwinTransformerBlock</code></h2> <h3 id="window-partition-reverse"><a href="#window-partition-reverse" class="header-anchor">#</a> Window Partition/Reverse</h3> <p>在开始讲主体模块前，有必要先讲Window Partition/Reverse机制，即函数<code>window_partition</code>和<code>window_reverse</code>。</p> <blockquote><p><code>window_partition</code>函数是用于对张量划分窗口，指定窗口大小。将原本的张量从 <code>N</code>x<code>H</code>x<code>W</code>x<code>C</code>, 划分成 <code>num_windows*B</code>x<code>window_size</code>x<code>window_size</code>x<code>C</code>，其中 <code>num_windows</code> = <code>H*W / (window_size*window_size)</code>，即窗口的个数。而<code>window_reverse</code>函数则是对应的逆过程。这两个函数会在后面的Window Attention用到。</p></blockquote> <p><code>window_partition</code>函数长这样：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">window_partition</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> window_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Args:
        x: (B, H, W, C)
        window_size (int): window size
    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    &quot;&quot;&quot;</span>
    B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H <span class="token operator">//</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> W <span class="token operator">//</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
    windows <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
    <span class="token keyword">return</span> windows
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>可以看到：</p> <ol><li>首先是<code>view</code>将输入的mask中的长宽维扩出来一个<code>window_size</code>维度，即从<code>B</code>x<code>H</code>x<code>W</code>x<code>C</code>扩成<code>B</code>x<code>H/window_size</code>x<code>window_size</code>x<code>W/window_size</code>x<code>window_size</code>x<code>C</code></li> <li>然后<code>permute</code>在<code>W/window_size</code>和<code>window_size</code>维度进行转置，变成<code>B</code>x<code>H/window_size</code>x<code>W/window_size</code>x<code>window_size</code>x<code>window_size</code>x<code>C</code></li> <li><code>view</code>和<code>permute</code>都不改变原始矩阵中的值，而用<code>contiguous</code>是把转置后的矩阵拷贝一份</li> <li>最后的<code>view(-1, window_size, window_size, C)</code>把<code>B</code>x<code>H/window_size</code>x<code>W/window_size</code>这几个维度都抹了，只剩下一堆<code>window_size</code>x<code>window_size</code>x<code>C</code></li></ol> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">window_reverse</span><span class="token punctuation">(</span>windows<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span><span class="token punctuation">:</span>
    B <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>windows<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token punctuation">(</span>H <span class="token operator">*</span> W <span class="token operator">/</span> window_size <span class="token operator">/</span> window_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> windows<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H <span class="token operator">//</span> window_size<span class="token punctuation">,</span> W <span class="token operator">//</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><h3 id="初始化-2"><a href="#初始化-2" class="header-anchor">#</a> 初始化</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">SwinTransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r&quot;&quot;&quot; Swin Transformer Block.
    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> input_resolution<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> window_size<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> shift_size<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
                 mlp_ratio<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> qkv_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> qk_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> drop<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> attn_drop<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> drop_path<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>
                 act_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>GELU<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
        self<span class="token punctuation">.</span>input_resolution <span class="token operator">=</span> input_resolution
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>window_size <span class="token operator">=</span> window_size
        self<span class="token punctuation">.</span>shift_size <span class="token operator">=</span> shift_size
        self<span class="token punctuation">.</span>mlp_ratio <span class="token operator">=</span> mlp_ratio
        <span class="token keyword">if</span> <span class="token builtin">min</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_resolution<span class="token punctuation">)</span> <span class="token operator">&lt;=</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">:</span>
            <span class="token comment"># if window size is larger than input resolution, we don't partition windows</span>
            self<span class="token punctuation">.</span>shift_size <span class="token operator">=</span> <span class="token number">0</span>
            self<span class="token punctuation">.</span>window_size <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_resolution<span class="token punctuation">)</span>
        <span class="token keyword">assert</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> self<span class="token punctuation">.</span>shift_size <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> <span class="token string">&quot;shift_size must in 0-window_size&quot;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br></div></div><p>这个<code>WindowAttention</code>显然就是论文里讲的在窗口内的Attention，后面细讲。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> WindowAttention<span class="token punctuation">(</span>
            dim<span class="token punctuation">,</span> window_size<span class="token operator">=</span>to_2tuple<span class="token punctuation">(</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span><span class="token punctuation">,</span> num_heads<span class="token operator">=</span>num_heads<span class="token punctuation">,</span>
            qkv_bias<span class="token operator">=</span>qkv_bias<span class="token punctuation">,</span> qk_scale<span class="token operator">=</span>qk_scale<span class="token punctuation">,</span> attn_drop<span class="token operator">=</span>attn_drop<span class="token punctuation">,</span> proj_drop<span class="token operator">=</span>drop<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>下面这是个带Dropout的全连接层，不多讲。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        self<span class="token punctuation">.</span>drop_path <span class="token operator">=</span> DropPath<span class="token punctuation">(</span>drop_path<span class="token punctuation">)</span> <span class="token keyword">if</span> drop_path <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token keyword">else</span> nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>
        mlp_hidden_dim <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>dim <span class="token operator">*</span> mlp_ratio<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> Mlp<span class="token punctuation">(</span>in_features<span class="token operator">=</span>dim<span class="token punctuation">,</span> hidden_features<span class="token operator">=</span>mlp_hidden_dim<span class="token punctuation">,</span> act_layer<span class="token operator">=</span>act_layer<span class="token punctuation">,</span> drop<span class="token operator">=</span>drop<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h4 id="shift-windows核心代码"><a href="#shift-windows核心代码" class="header-anchor">#</a> Shift Windows核心代码</h4> <p>所谓Shift Windows核心就是下面这段代码。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>shift_size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h5 id="attention-mask-for-sw-msa"><a href="#attention-mask-for-sw-msa" class="header-anchor">#</a> Attention mask for SW-MSA</h5> <p>首先是“calculate attention mask for SW-MSA”？</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>            <span class="token comment"># calculate attention mask for SW-MSA</span>

            H<span class="token punctuation">,</span> W <span class="token operator">=</span> self<span class="token punctuation">.</span>input_resolution
            img_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 1 H W 1</span>
            h_slices <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                        <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                        <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            w_slices <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                        <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
                        <span class="token builtin">slice</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            cnt <span class="token operator">=</span> <span class="token number">0</span>
            <span class="token keyword">for</span> h <span class="token keyword">in</span> h_slices<span class="token punctuation">:</span>
                <span class="token keyword">for</span> w <span class="token keyword">in</span> w_slices<span class="token punctuation">:</span>
                    img_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> cnt
                    cnt <span class="token operator">+=</span> <span class="token number">1</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>从两个<code>for</code>循环看，这个<code>img_mask</code>应该是一个标记，标记了每个待滑动区域的编号。实际上等价于：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>img_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>            <span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span>window_size<span class="token punctuation">,</span>            <span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span>window_size<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
img_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>            <span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span>window_size<span class="token punctuation">,</span> <span class="token operator">-</span>window_size<span class="token punctuation">:</span> <span class="token operator">-</span>shift_size<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
img_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>            <span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span>window_size<span class="token punctuation">,</span> <span class="token operator">-</span>shift_size <span class="token punctuation">:</span>            <span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">2</span>
img_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>window_size<span class="token punctuation">:</span> <span class="token operator">-</span>shift_size<span class="token punctuation">,</span>            <span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span>window_size<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">3</span>
img_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>window_size<span class="token punctuation">:</span> <span class="token operator">-</span>shift_size<span class="token punctuation">,</span> <span class="token operator">-</span>window_size<span class="token punctuation">:</span> <span class="token operator">-</span>shift_size<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">4</span>
img_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>window_size<span class="token punctuation">:</span> <span class="token operator">-</span>shift_size<span class="token punctuation">,</span> <span class="token operator">-</span>shift_size <span class="token punctuation">:</span>            <span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">5</span>
img_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>shift_size <span class="token punctuation">:</span>            <span class="token punctuation">,</span>            <span class="token number">0</span><span class="token punctuation">:</span><span class="token operator">-</span>window_size<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">6</span>
img_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>shift_size <span class="token punctuation">:</span>            <span class="token punctuation">,</span> <span class="token operator">-</span>window_size<span class="token punctuation">:</span> <span class="token operator">-</span>shift_size<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">7</span>
img_mask<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>shift_size <span class="token punctuation">:</span>            <span class="token punctuation">,</span> <span class="token operator">-</span>shift_size <span class="token punctuation">:</span>            <span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">8</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>标记大概是这样：
<img src="i/SwinTransformerShift.svg" alt=""></p> <p>根据调用链进行查找，可以发现<code>window_size</code>是用户指定的，而<code>input_resolution</code>和<code>shift_size</code>都是计算得到：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>input_resolution<span class="token operator">=</span><span class="token punctuation">(</span>patches_resolution<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">**</span> i_layer<span class="token punctuation">)</span><span class="token punctuation">,</span> patches_resolution<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">**</span> i_layer<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>其中<code>i_layer</code>是<code>BasicLayer</code>的编号，说明这个<code>input_resolution</code>逐层减半，和论文说的一样。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>shift_size<span class="token operator">=</span><span class="token number">0</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">else</span> window_size <span class="token operator">//</span> <span class="token number">2</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>其中<code>i</code>是<code>SwinTransformerBlock</code>在<code>BasicLayer</code>中的编号，说明这个shift操作是隔层进行的，对应论文中的这个图：</p> <p><img src="i/SwinTransformerBlock.png" alt=""></p> <p>即每两个<code>SwinTransformerBlock</code>组成一个基本单元，中间有一个Shift操作。</p> <p>这也说明了W-MSA实际上只是SW-MSA少了一个Shift操作。</p> <p>接下来是一个<code>window_partition</code>操作：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>            mask_windows <span class="token operator">=</span> window_partition<span class="token punctuation">(</span>img_mask<span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span>  <span class="token comment"># nW, window_size, window_size, 1</span>
            mask_windows <span class="token operator">=</span> mask_windows<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>然后给填了一下数据？-100和0？</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>            attn_mask <span class="token operator">=</span> mask_windows<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> mask_windows<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
            attn_mask <span class="token operator">=</span> attn_mask<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>attn_mask <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">100.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>attn_mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            attn_mask <span class="token operator">=</span> <span class="token boolean">None</span>

        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">&quot;attn_mask&quot;</span><span class="token punctuation">,</span> attn_mask<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到是对上面那个mask进行的操作。但是这到底是什么意思呢？</p> <h3 id="推断过程-3"><a href="#推断过程-3" class="header-anchor">#</a> 推断过程</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        H<span class="token punctuation">,</span> W <span class="token operator">=</span> self<span class="token punctuation">.</span>input_resolution
        B<span class="token punctuation">,</span> L<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        <span class="token keyword">assert</span> L <span class="token operator">==</span> H <span class="token operator">*</span> W<span class="token punctuation">,</span> <span class="token string">&quot;input feature has wrong size&quot;</span>

        shortcut <span class="token operator">=</span> x
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>这里一个shift操作，直接用<code>torch.roll</code>完成。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token comment"># cyclic shift</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>shift_size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            shifted_x <span class="token operator">=</span> torch<span class="token punctuation">.</span>roll<span class="token punctuation">(</span>x<span class="token punctuation">,</span> shifts<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span> dims<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            shifted_x <span class="token operator">=</span> x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p><code>torch.roll</code>的原理是这样：
<img src="i/torch.roll.jpg" alt=""></p> <p>那显然这里对应论文里的这个Cyclic Shift操作：
<img src="i/SwinTransformerShiftCyclic.png" alt=""></p> <p>但这个和前面的<code>img_mask</code>又有什么关系？</p> <p>接下来是<code>window_partition</code>把输入特征图切成windows，不用多讲。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token comment"># partition windows</span>
        x_windows <span class="token operator">=</span> window_partition<span class="token punctuation">(</span>shifted_x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">)</span>  <span class="token comment"># nW*B, window_size, window_size, C</span>
        x_windows <span class="token operator">=</span> x_windows<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> C<span class="token punctuation">)</span>  <span class="token comment"># nW*B, window_size*window_size, C</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>再然后就是应用<code>WindowAttention</code>：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token comment"># W-MSA/SW-MSA</span>
        attn_windows <span class="token operator">=</span> self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>x_windows<span class="token punctuation">,</span> mask<span class="token operator">=</span>self<span class="token punctuation">.</span>attn_mask<span class="token punctuation">)</span>  <span class="token comment"># nW*B, window_size*window_size, C</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>再然后就是把切成windows的特征图还原回来：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token comment"># merge windows</span>
        attn_windows <span class="token operator">=</span> attn_windows<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
        shifted_x <span class="token operator">=</span> window_reverse<span class="token punctuation">(</span>attn_windows<span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">)</span>  <span class="token comment"># B H' W' C</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>再然后就是把移位后的特征图还原回来：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token comment"># reverse cyclic shift</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>shift_size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>roll<span class="token punctuation">(</span>shifted_x<span class="token punctuation">,</span> shifts<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>shift_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>shift_size<span class="token punctuation">)</span><span class="token punctuation">,</span> dims<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> shifted_x
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B<span class="token punctuation">,</span> H <span class="token operator">*</span> W<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>最后处理一下Dropout：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        x <span class="token operator">=</span> shortcut <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># FFN</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>drop_path<span class="token punctuation">(</span>self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><h3 id="总结"><a href="#总结" class="header-anchor">#</a> 总结</h3> <p>总结一下，这里显然是Swin Transformer中各种Shift Windows的核心代码，但是看完了还是没能理解这个Shift Windows到底是在干什么。且看看后面的代码在回头总结吧。</p> <h2 id="核心模块windowattention"><a href="#核心模块windowattention" class="header-anchor">#</a> 核心模块<code>WindowAttention</code></h2> <h3 id="初始化-3"><a href="#初始化-3" class="header-anchor">#</a> 初始化</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">WindowAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim<span class="token punctuation">,</span> window_size<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> qkv_bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> qk_scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> attn_drop<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> proj_drop<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
        self<span class="token punctuation">.</span>window_size <span class="token operator">=</span> window_size  <span class="token comment"># Wh, Ww</span>
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        head_dim <span class="token operator">=</span> dim <span class="token operator">//</span> num_heads
        self<span class="token punctuation">.</span>scale <span class="token operator">=</span> qk_scale <span class="token keyword">or</span> head_dim <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p>一大段的相对位置编码：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token comment"># define a parameter table of relative position bias</span>
        self<span class="token punctuation">.</span>relative_position_bias_table <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>
            torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 2*Wh-1 * 2*Ww-1, nH</span>
            
        <span class="token comment"># get pair-wise relative position index for each token inside the window</span>
        coords_h <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        coords_w <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        coords <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>meshgrid<span class="token punctuation">(</span><span class="token punctuation">[</span>coords_h<span class="token punctuation">,</span> coords_w<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 2, Wh, Ww</span>
        coords_flatten <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>coords<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 2, Wh*Ww</span>
        relative_coords <span class="token operator">=</span> coords_flatten<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span> <span class="token operator">-</span> coords_flatten<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># 2, Wh*Ww, Wh*Ww</span>
        relative_coords <span class="token operator">=</span> relative_coords<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># Wh*Ww, Wh*Ww, 2</span>
        relative_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>  <span class="token comment"># shift to start from 0</span>
        relative_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>
        relative_coords<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>
        relative_position_index <span class="token operator">=</span> relative_coords<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># Wh*Ww, Wh*Ww</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">&quot;relative_position_index&quot;</span><span class="token punctuation">,</span> relative_position_index<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>几个线性层和Dropout啥的，不必多讲</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        self<span class="token punctuation">.</span>qkv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim <span class="token operator">*</span> <span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>qkv_bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attn_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>attn_drop<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj_drop <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>proj_drop<span class="token punctuation">)</span>

        trunc_normal_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>relative_position_bias_table<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">.02</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="推断过程-4"><a href="#推断过程-4" class="header-anchor">#</a> 推断过程</h3> <div class="language-python line-numbers-mode"><pre class="language-python"><code>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        &quot;&quot;&quot;</span>
        B_<span class="token punctuation">,</span> N<span class="token punctuation">,</span> C <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        qkv <span class="token operator">=</span> self<span class="token punctuation">.</span>qkv<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B_<span class="token punctuation">,</span> N<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> C <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
        q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v <span class="token operator">=</span> qkv<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> qkv<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>  <span class="token comment"># make torchscript happy (cannot use tensor as tuple)</span>

        q <span class="token operator">=</span> q <span class="token operator">*</span> self<span class="token punctuation">.</span>scale
        attn <span class="token operator">=</span> <span class="token punctuation">(</span>q @ k<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>这里加上了相对位置编码：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        relative_position_bias <span class="token operator">=</span> self<span class="token punctuation">.</span>relative_position_bias_table<span class="token punctuation">[</span>self<span class="token punctuation">.</span>relative_position_index<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>window_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># Wh*Ww,Wh*Ww,nH</span>
        relative_position_bias <span class="token operator">=</span> relative_position_bias<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># nH, Wh*Ww, Wh*Ww</span>
        attn <span class="token operator">=</span> attn <span class="token operator">+</span> relative_position_bias<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>然后加上了mask？按照前面讲的，这个mask里面是-100和0，所以这里是什么意思？</p> <blockquote><p>将mask加到attention的计算结果，并进行softmax。mask的值设置为-100，softmax后就会忽略掉对应的值</p></blockquote> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            nW <span class="token operator">=</span> mask<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>view<span class="token punctuation">(</span>B_ <span class="token operator">//</span> nW<span class="token punctuation">,</span> nW<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> N<span class="token punctuation">,</span> N<span class="token punctuation">)</span> <span class="token operator">+</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            attn <span class="token operator">=</span> attn<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> N<span class="token punctuation">,</span> N<span class="token punctuation">)</span>
            attn <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            attn <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>处理Dropout：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        attn <span class="token operator">=</span> self<span class="token punctuation">.</span>attn_drop<span class="token punctuation">(</span>attn<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>处理最后一个线性层和Dropout：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>        x <span class="token operator">=</span> <span class="token punctuation">(</span>attn @ v<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>B_<span class="token punctuation">,</span> N<span class="token punctuation">,</span> C<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>proj_drop<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h3 id="总结-2"><a href="#总结-2" class="header-anchor">#</a> 总结</h3> <p>前面那个mask居然是在这里用的？还是没懂。</p> <h2 id="总结-3"><a href="#总结-3" class="header-anchor">#</a> 总结</h2> <p><img src="i/SwinTransformerShiftWindow.png" alt=""></p> <p>还是看看大佬总结的吧<a href="/人工智能/图解SwinTransformer.html">《图解Swin Transformer》</a></p></div> <footer class="page-edit" style="display:none;"><div class="edit-link"><a href="https://github.com/yindaheng98/Notebook/edit/master/学习笔记/人工智能/SwinTransformer.md" target="_blank" rel="noopener noreferrer">帮助我们改善此页面！</a> <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></div> <div class="last-updated"><span class="prefix">创建于: </span> <span class="time">2022-06-12 07:36:34</span></div> <br> <div class="last-updated"><span class="prefix">更新于: </span> <span class="time">2022-06-12 12:22:21</span></div></footer> <!----> <!----></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-44bd5a18 data-v-44bd5a18><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-44bd5a18><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-44bd5a18></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-44bd5a18></path></svg></div></div></div>
    <script src="/assets/js/app.1c89d7eb.js" defer></script><script src="/assets/js/3.63b8c0eb.js" defer></script><script src="/assets/js/1.be3d2055.js" defer></script><script src="/assets/js/145.0826322f.js" defer></script>
  </body>
</html>
