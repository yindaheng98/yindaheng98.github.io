(window.webpackJsonp=window.webpackJsonp||[]).push([[155],{828:function(e,t,a){"use strict";a.r(t);var s=a(4),o=Object(s.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[a("img",{attrs:{src:"i/20240921222730.png",alt:""}})]),e._v(" "),a("h2",{attrs:{id:"ransac算法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ransac算法"}},[e._v("#")]),e._v(" RANSAC算法")]),e._v(" "),a("p",[e._v("RANSAC算法的基本步骤")]),e._v(" "),a("table",[a("thead",[a("tr",[a("th",[e._v("普通步骤")]),e._v(" "),a("th",[e._v("视觉SLAM中的步骤")])])]),e._v(" "),a("tbody",[a("tr",[a("td",[e._v("确定所需的最小样本数量，这取决于模型类型（例如，对于线性模型至少需要2个点，对于平面至少需要3个点）")]),e._v(" "),a("td",[e._v("八点法构造本质矩阵或基础矩阵以求解模型参数，所以需要取8对匹配的关键点")])]),e._v(" "),a("tr",[a("td",[e._v("随机抽样")]),e._v(" "),a("td",[e._v("随机从已有的匹配特征点对中选取八队")])]),e._v(" "),a("tr",[a("td",[e._v("模型拟合，取得参数")]),e._v(" "),a("td",[e._v("获得针对该八个点对的R和T")])]),e._v(" "),a("tr",[a("td",[e._v("设置一致性阈值，进行一致性检查")]),e._v(" "),a("td",[e._v("遍历其他所有匹配点对，将和模型参数一致（通过刚刚的R和T可以得到第二帧的对应特征点位置）的作为内点，否则作为外点处理")])]),e._v(" "),a("tr",[a("td",[e._v("迭代")]),e._v(" "),a("td",[e._v("不断迭代，得到一个又一个的内点集和外点集")])]),e._v(" "),a("tr",[a("td",[e._v("模型确定")]),e._v(" "),a("td",[e._v("内点集最多的8个点对得到的R和T作为最终模型")])])])]),e._v(" "),a("h2",{attrs:{id:"cvpr-2019-nocs-normalized-object-coordinate-space-for-category-level-6d-object-pose-and-size-estimation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cvpr-2019-nocs-normalized-object-coordinate-space-for-category-level-6d-object-pose-and-size-estimation"}},[e._v("#")]),e._v(" (CVPR 2019) NOCS: Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation")]),e._v(" "),a("p",[e._v("过去的6D姿态估计方法主要有两个缺陷;首先需要有该对象的CAD模型并且已知对象的尺寸，这对于大部分未见过的对象无法使用。另外一种虽然可以不需要已有的CAD模型，但是依赖于视点，不会对目标对象的精确方向进行编码。所以这两类方法都无法对新环境中的对象进行精确的姿态估计。")]),e._v(" "),a("p",[a("img",{attrs:{src:"i/20240921230845.png",alt:""}})]),e._v(" "),a("p",[e._v("论文中定义了一种共享空间（归一化对象坐标空间）：NOCS（一个包含在单位立方体中的三维空间，将对角线设置为1，保证对象收缩在单位立方体中）如图，所有对象都包含在一个公共的规范化空间中，一个类别中的所有实例享有一个共同的参考框架。")]),e._v(" "),a("p",[e._v("类级别的目标6D姿态与尺寸估计：首先6D姿态是指对物体3D位置与3D方向的估计。（3个自由度定位置，三个自由度定旋转方向）,其中方向的表示方法主要有三种，欧拉角，旋转矩阵，四元数。类级别的6D姿态估计是指对一个未见过的目标对象进行估计，但是这个对象属于某一类别（如相机），而在之前的训练中训练样本含有相机，那么便可以对该目标对象进行估计。")]),e._v(" "),a("p",[e._v("NOCS map:通过训练一个CNN预测一个彩色编码的NOCS目标的的二维透视投影（上图左下角），NOCS map能提供在规范空间的物体的外形和尺寸。")]),e._v(" "),a("p",[e._v("网络结构如下：网络以单张包含多个对象的RGB图像和对应的depth图作为输入，使用CNN来预测对象的类标签、掩码和NOCS map。然后利用NOCS map和深度图利用位姿拟合的方法来估计物体的完整6D姿态和尺寸。")]),e._v(" "),a("p",[a("img",{attrs:{src:"i/20240921230959.png",alt:""}})]),e._v(" "),a("h2",{attrs:{id:"iccv-2019-有3d模型的位姿估计-cdpn-coordinates-based-disentangled-pose-network-for-real-time-rgb-based-6-dof-object-pose-estimation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#iccv-2019-有3d模型的位姿估计-cdpn-coordinates-based-disentangled-pose-network-for-real-time-rgb-based-6-dof-object-pose-estimation"}},[e._v("#")]),e._v(" (ICCV 2019, 有3D模型的位姿估计) CDPN: Coordinates-Based Disentangled Pose Network for Real-Time RGB-Based 6-DoF Object Pose Estimation")]),e._v(" "),a("p",[e._v("作者认为旋转和平移的显着差异应区别对待。 这项工作提出了一种新颖的6自由度姿势估计方法：基于坐标的解缠姿势网络（CDPN，Coordinates-based Disentangled Pose Network），该方法可以解开姿势"),a("strong",[e._v("分别预测旋转和平移")]),e._v("，实现高度准确和鲁棒的姿势估计。这种方法灵活和高效，可以处理没有纹理和被遮挡的目标。")]),e._v(" "),a("p",[e._v("其步骤如图所示，给定输入图像，先放大目标，然后解开旋转和平移以进行估计。 具体而言，旋转是通过PnP从预测的3D坐标中解决的，而平移则直接从图像中估算出来。")]),e._v(" "),a("p",[a("img",{attrs:{src:"i/20240921225355.png",alt:""}})]),e._v(" "),a("h2",{attrs:{id:"iccv-2019-有3d模型的位姿估计-dpod-6d-pose-object-detector-and-refiner"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#iccv-2019-有3d模型的位姿估计-dpod-6d-pose-object-detector-and-refiner"}},[e._v("#")]),e._v(" (ICCV 2019, 有3D模型的位姿估计) DPOD: 6D Pose Object Detector and Refiner")]),e._v(" "),a("p",[e._v("本文提出了一种仅从RGB图像进行3D目标检测和6D姿态估计的深度学习方法。此方法称为（DPOD，Dense Pose Object Detector）密集姿势目标检测器，用于估计输入图像和可用3D模型之间的密集多类2D-3D对应图。给定对应关系，可通过PnP和RANSAC计算6DoF姿态。基于深度学习的定制细化方案对初始姿态估计值进行细化。")]),e._v(" "),a("p",[a("img",{attrs:{src:"i/20240921223058.png",alt:""}})]),e._v(" "),a("p",[e._v("把物体的材质换成Correspondence Mapping")]),e._v(" "),a("h2",{attrs:{id:"cvpr-2023-无3d模型的位姿估计-nerf-pose-a-first-reconstruct-then-regress-approach-for-weakly-supervised-6d-object-pose-estimation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cvpr-2023-无3d模型的位姿估计-nerf-pose-a-first-reconstruct-then-regress-approach-for-weakly-supervised-6d-object-pose-estimation"}},[e._v("#")]),e._v(" (CVPR 2023, 无3D模型的位姿估计) NeRF-Pose: A First-Reconstruct-Then-Regress Approach for  Weakly-supervised 6D Object Pose Estimation")]),e._v(" "),a("p",[e._v("NeRF用于物体的3D姿态估计，以前的3D姿态估计需要给出物体的3D模型，而本文只需要物体的多视角照片和Mask")]),e._v(" "),a("p",[a("img",{attrs:{src:"i/20240921221446.png",alt:""}})]),e._v(" "),a("h3",{attrs:{id:"stage-1-multi-view-neural-object-reconstruction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#stage-1-multi-view-neural-object-reconstruction"}},[e._v("#")]),e._v(" Stage 1: Multi-view Neural Object Reconstruction")]),e._v(" "),a("p",[e._v("用物体的多视角照片和目标分割Mask作为训练数据训练一个NeRF")]),e._v(" "),a("h3",{attrs:{id:"stage-2-single-view-object-pose-estimation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#stage-2-single-view-object-pose-estimation"}},[e._v("#")]),e._v(" Stage 2: Single View Object Pose Estimation")]),e._v(" "),a("ol",[a("li",[e._v("传统目标识别抠出目标区域")]),e._v(" "),a("li",[e._v("Coordinates regression(基本上就是传统DPoD/CDPN)输出NOCS和目标分割Mask")]),e._v(" "),a("li",[e._v("传统RANSAC算法随机采样进行Pose Regression，对每个采样视角：\n"),a("ol",[a("li",[e._v("渲染：在该视角处NeRF渲染NOCS和目标分割Mask")]),e._v(" "),a("li",[e._v("比对：NeRF渲染结果 vs 上一步Coordinates regression输出的NOCS和目标分割Mask\n"),a("ul",[a("li",[e._v("NeRF渲染的NOCS vs Coordinates regression输出的NOCS：Inlier Ratio")]),e._v(" "),a("li",[e._v("NeRF渲染的目标分割Mask vs Coordinates regression输出的目标分割Mask：Recall和Precision")])])])])])]),e._v(" "),a("p",[a("img",{attrs:{src:"i/20240921222156.png",alt:""}})]),e._v(" "),a("h2",{attrs:{id:"cvpr-2024-大模型增强数据训练出通用位姿估计-foundationpose-unified-6d-pose-estimation-and-tracking-of-novel-objects"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#cvpr-2024-大模型增强数据训练出通用位姿估计-foundationpose-unified-6d-pose-estimation-and-tracking-of-novel-objects"}},[e._v("#")]),e._v(" (CVPR 2024, 大模型增强数据训练出通用位姿估计) FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects")]),e._v(" "),a("p",[a("img",{attrs:{src:"i/20240921224150.png",alt:""}})]),e._v(" "),a("p",[e._v("其实最根本性的创新在大模型数据增强那里，其他的：")]),e._v(" "),a("ul",[a("li",[e._v("Neural Object Modeling是NeRF-Pose里的已有方法")]),e._v(" "),a("li",[e._v("Pose Hypothesis Generation是用Transformer替换了CDPN里的CNN输出平移和旋转，本文会输出多组位姿假设供Pose Selection选择")]),e._v(" "),a("li",[e._v("Pose Selection：用Transformer给Pose Hypothesis Generation输出的多组位姿假设打分，取最高分")])]),e._v(" "),a("h3",{attrs:{id:"_3-1-language-aided-data-generation-at-scale-用大模型增强训练数据"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-language-aided-data-generation-at-scale-用大模型增强训练数据"}},[e._v("#")]),e._v(" 3.1. Language-aided Data Generation at Scale 用大模型增强训练数据")]),e._v(" "),a("p",[e._v("已有一些有标注的数据（3D模型和对应标签），用大语言模型进行数据强化：")]),e._v(" "),a("ol",[a("li",[e._v("LLM转文字描述（”describe its possible appearance”生成大量不同外观的文字描述）")]),e._v(" "),a("li",[e._v("文字输入Diffusion生成大量不同外观的模型")]),e._v(" "),a("li",[e._v("物理引擎模拟模型轨迹")])]),e._v(" "),a("p",[e._v("从而得到大量训练物体轨迹数据用于训练")]),e._v(" "),a("p",[a("img",{attrs:{src:"i/20240921223808.png",alt:""}})]),e._v(" "),a("h3",{attrs:{id:"针对无3d模型输入的情况-neural-object-modeling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#针对无3d模型输入的情况-neural-object-modeling"}},[e._v("#")]),e._v(" (针对无3D模型输入的情况) Neural Object Modeling")]),e._v(" "),a("p",[e._v("NeRF-Pose里的已有方法，就是做了点微小的创新，把传统NeRF分成了Geometry和Appearance两个NeRF")]),e._v(" "),a("h3",{attrs:{id:"pose-hypothesis-generation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pose-hypothesis-generation"}},[e._v("#")]),e._v(" Pose Hypothesis Generation")]),e._v(" "),a("p",[e._v("随机采样多个视角然后用类似CDPN的方法进行Pose Refinement，会输出多组位姿假设（局部最优解）")]),e._v(" "),a("h3",{attrs:{id:"pose-selection"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pose-selection"}},[e._v("#")]),e._v(" Pose Selection")]),e._v(" "),a("p",[e._v("给定一系列姿态假设，我们使用分层姿态排序网络来计算它们的得分。\n得分最高的姿态被选为最终估计值。")]),e._v(" "),a("p",[e._v("分层姿态排序网络：如果直接输入一个姿态输出一个分，this would ignore the other pose hypotheses, forcing the network to output an absolute score assignment which can be difficult to learn.\n所以还有一层用来rank among all the K pose hypotheses.")]),e._v(" "),a("p",[a("img",{attrs:{src:"i/20240921225925.png",alt:""}})])])}),[],!1,null,null,null);t.default=o.exports}}]);