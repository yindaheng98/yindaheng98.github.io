(window.webpackJsonp=window.webpackJsonp||[]).push([[132],{708:function(e,t,a){"use strict";a.r(t);var r=a(4),n=function(e){e.options.__data__block__={mermaid_382ee168:"graph LR\nRDMA(RDMA)--\x3e技术分类(技术分类)\n技术分类--\x3eInfiniband(Infiniband)\n技术分类--\x3eRoCE(RoCE)\n技术分类--\x3eiWARP(iWARP)\nRDMA--\x3e相关应用(相关应用)\n相关应用--\x3ePolarDB(PolarDB)\n相关应用--\x3ePolarFS(PolarFS)\n相关应用--\x3eGlusterFS(GlusterFS)\n"}},i=Object(r.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[e._v("RDMA（Remote Direct Memory Access）技术是一种"),a("strong",[e._v("直接在内存和内存之间进行资料互传")]),e._v("的技术，在数据传输的过程中，"),a("strong",[e._v("不需要和操作系统Kernel做沟通")]),e._v("，完全实现了Kernel Bypass。")]),e._v(" "),a("p",[e._v("在TCP/IP中，从应用层生成TCP的packet、对packet添加crc、运行TCP协议传输数据到接收端，如果是可靠连接网络，还有流量的控制、查询内存地址、检验crc、还原TCP packet到应用，这些操作都是在CPU内完成的，而在RDMA中，这些操作都在网卡中完成，CPU不参与，这就是为什么RDMA能带来低延时、高带宽和低CPU利用率的原因。")]),e._v(" "),a("p",[e._v("相关资料：")]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://www.zhihu.com/question/59122163",target:"_blank",rel:"noopener noreferrer"}},[e._v("在各互联网公司中，有将 RDMA 技术用于生产环境的实例吗？"),a("OutboundLink")],1)]),e._v(" "),a("li",[a("a",{attrs:{href:"https://www.zhihu.com/question/63987114",target:"_blank",rel:"noopener noreferrer"}},[e._v("如何评价阿里云新一代关系型数据库 PolarDB？"),a("OutboundLink")],1)])]),e._v(" "),a("h2",{attrs:{id:"知识图谱"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#知识图谱"}},[e._v("#")]),e._v(" 知识图谱")]),e._v(" "),a("Mermaid",{attrs:{id:"mermaid_382ee168",graph:e.$dataBlock.mermaid_382ee168}}),a("h2",{attrs:{id:"技术分类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#技术分类"}},[e._v("#")]),e._v(" 技术分类")]),e._v(" "),a("p",[e._v("主要技术公司：以色列Mellanox公司（IBM控股）")]),e._v(" "),a("h3",{attrs:{id:"infiniband"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#infiniband"}},[e._v("#")]),e._v(" InfiniBand")]),e._v(" "),a("ul",[a("li",[e._v("RDMA之正统协议，RDMA未来的绝对主流")]),e._v(" "),a("li",[e._v("RDMA的最佳实践方案")]),e._v(" "),a("li",[e._v("与TCP/IP硬件完全不兼容，不支持以太网交换机，必须要有专有硬件，因此成本很高")])]),e._v(" "),a("h3",{attrs:{id:"roce-rdma-over-converged-ethernet"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#roce-rdma-over-converged-ethernet"}},[e._v("#")]),e._v(" RoCE(RDMA over Converged Ethernet)")]),e._v(" "),a("ul",[a("li",[e._v("因为IB过于昂贵所以出了个性能不太好的低价版RDMA方案")]),e._v(" "),a("li",[e._v("把IB的包放到通用Ethernet上面发出去，支持以太网交换机，只需要支持RoCE协议的网卡即可使用")]),e._v(" "),a("li",[e._v("市场上常见的RDMA大都是这个方案")])]),e._v(" "),a("h3",{attrs:{id:"iwarp"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#iwarp"}},[e._v("#")]),e._v(" iWARP")]),e._v(" "),a("p",[e._v("在TCP/IP的基础上实现的RDMA，基本上算是TCP/IP向RoCE的过渡版本，因为性能太差所以正在逐渐淘汰。")]),e._v(" "),a("h3",{attrs:{id:"技术本质"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#技术本质"}},[e._v("#")]),e._v(" 技术本质")]),e._v(" "),a("p",[e._v("其实不管是iWARP还是RoCE，实际上并不是自己重新发明了RDMA，而是"),a("strong",[e._v("利用了IB的上层接口修改了下层的实现")]),e._v("，所以RoCE这个名字并不是很准确，比较准确的说法应该是IB over Converged Ethernet。此外，三种实现方式使用的user space api是一样的，都是libibverbs，这个库原本也就是给IB用的，相当于IB的socket。")]),e._v(" "),a("h2",{attrs:{id:"应用领域"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#应用领域"}},[e._v("#")]),e._v(" 应用领域")]),e._v(" "),a("h3",{attrs:{id:"高性能计算"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#高性能计算"}},[e._v("#")]),e._v(" 高性能计算")]),e._v(" "),a("p",[e._v("关于市场，实际上RDMA的市场一直都不算太小，传统的IB主要面向的是HPC（High Performance Compute，高性能计算），比如分布式的人工智能和大数据分析等，HP和IBM一直在使用，但是毕竟HPC只是企业级里面一块很小的业务，也不是什么企业都需要或者用得起HPC的。现在比较值得注意的一点是Intel实际上提出来了一种概念叫做“新型RDMA应用”，即传统的RDMA应用对延迟(lat)和带宽(bw)都非常敏感，而新型RDMA应用则在lat上可以相对宽松，但是却要求在短时间内爆发式的单播或者广播(因为RDMA在协议栈层就支持可靠广播)大量的数据包。比较典型的一个应用是现在很火的大数据，IBM做了一个东西叫做"),a("a",{attrs:{href:"https://www.youtube.com/watch?v=t_4Ao2fNAfU",target:"_blank",rel:"noopener noreferrer"}},[e._v("Spark over RDMA"),a("OutboundLink")],1),e._v("。")]),e._v(" "),a("h3",{attrs:{id:"高速存储网络"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#高速存储网络"}},[e._v("#")]),e._v(" 高速存储网络")]),e._v(" "),a("p",[e._v("除了大数据之外，存储市场是将来RDMA发展的一个主要方向，事实上RDMA已经成为了下一代存储网络的事实标准。在10G网络上，如果用iscsi架构，iodepth=32，block-size=4k， random-read-write，TCP能实现的iops约为160～170k，这个数字对于一般的HDD阵列来讲已经足够，因为HDD阵列本身速度较慢。但是对于SSD单盘180k以上的iops，显然TCP性能是不足的，更遑论NVMe SSD单盘550k的ipos。因此，在SSD时代的存储网络中，RDMA几乎是一个必选项。")]),e._v(" "),a("h2",{attrs:{id:"inifiniband"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#inifiniband"}},[e._v("#")]),e._v(" InifiniBand")]),e._v(" "),a("p",[e._v("InfiniBand架构是一种支持多并发链接的“转换线缆”技术，它是新一代服务器硬件平台的I/O标准。由于它具有高带宽、低延时、 高可扩展性的特点，它非常适用于服务器与服务器（比如复制，分布式工作等），服务器和存储设备（比如SAN和直接存储附件）以及服务器和网络之间（比如LAN， WANs和the Internet）的通信。")]),e._v(" "),a("ul",[a("li",[e._v("Infiniband大量用于FC/IP SAN、NAS和服务器之间的连接,作为iSCSI RDMA的存储协议iSER已被IETF标准化。目前EMC全系产品已经切换到Infiniband组网，IBM/TMS的FlashSystem系列，IBM的存储系统XIV Gen3，DDN的SFA系列都采用Infiniband网络。")]),e._v(" "),a("li",[e._v("相比FC的优势主要体现在性能是FC的3.5倍，Infiniband交换机的延迟是FC交换机的1/10，支持SAN和NAS。")]),e._v(" "),a("li",[e._v("存储系统已不能满足于传统的FC SAN所提供的服务器与裸存储的网络连接架构。HP SFS和IBM GPFS 是在Infiniband fabric连接起来的服务器和iSER Infiniband存储构建的并行文件系统，完全突破系统的性能瓶颈。")]),e._v(" "),a("li",[e._v("Infiniband采用PCI串行高速带宽链接，从SDR、DDR、QDR、FDR到EDR HCA连接，可以做到1微妙、甚至纳米级别极低的时延，基于链路层的流控机制实现先进的拥塞控制。")]),e._v(" "),a("li",[e._v("InfiniBand采用虚通道(VL即Virtual Lanes)方式来实现QoS，虚通道是一些共享一条物理链接的相互分立的逻辑通信链路，每条物理链接可支持多达15条的标准虚通道和一条管理通道(VL15)。")])])],1)}),[],!1,null,null,null);"function"==typeof n&&n(i);t.default=i.exports}}]);