(window.webpackJsonp=window.webpackJsonp||[]).push([[139],{811:function(t,r,e){"use strict";e.r(r);var a=e(4),n=Object(a.a)({},(function(){var t=this,r=t.$createElement,e=t._self._c||r;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("iframe",{attrs:{width:"560",height:"315",src:"https://www.youtube.com/embed/DhRoTONcyZE?si=C-7G97hajia6lYR3",title:"YouTube video player",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",referrerpolicy:"strict-origin-when-cross-origin",allowfullscreen:""}}),t._v(" "),e("p",[t._v("随着预训练语言模型参数量越来越大，迁移学习的成本越来越高，parameter-efficient fine-tuning（参数高效的微调方法）成为一个热点研究方向。所谓“参数高效”，本质就是“节约参数”。在以前我们在下游任务使用预训练大模型，一般需要finetune模型的所有参数，每个下游任务都需要复制一份完整的模型参数。")]),t._v(" "),e("h2",{attrs:{id:"peft和lora的起源：adapters"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#peft和lora的起源：adapters"}},[t._v("#")]),t._v(" PEFT和LoRA的起源：Adapters")]),t._v(" "),e("p",[t._v("论文："),e("a",{attrs:{href:"https://arxiv.org/abs/1902.00751",target:"_blank",rel:"noopener noreferrer"}},[t._v("Parameter-Efficient Transfer Learning for NLP"),e("OutboundLink")],1),t._v("（2019.6）")]),t._v(" "),e("p",[t._v("Adapter是最早的一类PEFT方法的代表，2019年Houlsby N等人在这篇论文中首次提出了PEFT的概念并将Adapter引入NLP领域。Adapter主体架构下图所示：")]),t._v(" "),e("p",[e("img",{attrs:{src:"i/v2-bafe467a7ec7be6c3190b8ac19a79c03_1440w.webp",alt:""}})]),t._v(" "),e("p",[t._v("Adapter的本质就是一种Bottlenet，通过全连接对原输入进行降维，之后经过内部的神经网络后再来一个全连接将维度还原。")]),t._v(" "),e("p",[t._v("在finetune过程中，Bert模型中的每一层都加入这种Adapter结构，冻结住原预训练Bert模型的参数，只更新Adapter、layer norm和最后一层的参数，大大减少了finetune阶段需要更新和保存的参数量。")]),t._v(" "),e("p",[t._v("Adapter在NLU和NLG任务上有不错的性能表现，仅添加3.6%的参数就能达到接近full fine-tune的效果：")]),t._v(" "),e("p",[e("img",{attrs:{src:"i/v2-9159d2c260c68490a38bdecb373921b4_1440w.webp",alt:""}})]),t._v(" "),e("p",[t._v("但是，Adapter需要添加在每一层中训练，这意味着训练时的前向传播仍然需要保存大部分层的梯度供反向传播使用，虽然不用更新预训练参数从而减少训练时的计算量，但"),e("strong",[t._v("并不能节省训练时的显存占用")]),t._v("，还"),e("strong",[t._v("降低模型推理时的性能")]),t._v("。")]),t._v(" "),e("h2",{attrs:{id:"peft中的另一个世界：prompt-tuning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#peft中的另一个世界：prompt-tuning"}},[t._v("#")]),t._v(" PEFT中的另一个世界：Prompt Tuning")]),t._v(" "),e("p",[t._v("详阅："),e("RouterLink",{attrs:{to:"/人工智能/PromptTuning.html"}},[t._v("《【摘录】Prompt Tuning》")])],1),t._v(" "),e("p",[t._v("和LoRA在模型尾部做操作不同，Prompt Tuning是对模型的输入做操作，相当于对LLM的Prompt进行训练。\n但很显然，对模型输入做操作的finetune在前向传播时需要保存所有层的参数梯度，并不能节省训练时的显存占用。")]),t._v(" "),e("h2",{attrs:{id:"lora的原理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#lora的原理"}},[t._v("#")]),t._v(" LoRA的原理")]),t._v(" "),e("p",[t._v("有研究者对语言模型的参数进行研究发现：神经网络包含许多密集的层，这些层执行矩阵乘法。这些层中的权重矩阵通常具有全秩。"),e("strong",[t._v("当适应特定的下游任务时，预训练的语言模型往往具有较低的本征维度（instrisic dimension）")]),t._v("，尽管随机"),e("strong",[t._v("投影到较小的子空间")]),t._v("，但仍然可以有效地学习。换句话说，理论上我们可以微调非常小的参数量，就能在下游任务取得不错的效果。")]),t._v(" "),e("h2",{attrs:{id:"lora重参数化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#lora重参数化"}},[t._v("#")]),t._v(" LoRA重参数化")]),t._v(" "),e("p",[t._v("LoRA训练完成后，finetune过程中所使用的降维矩阵和升维矩阵可以直接加在原参数上，从而在推理时不会产生额外的推理时延。")]),t._v(" "),e("p",[e("img",{attrs:{src:"i/lora-rep.gif",alt:""}})])])}),[],!1,null,null,null);r.default=n.exports}}]);