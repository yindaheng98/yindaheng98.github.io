(window.webpackJsonp=window.webpackJsonp||[]).push([[304],{991:function(a,e,t){"use strict";t.r(e);var r=t(4),i=Object(r.a)({},(function(){var a=this,e=a.$createElement,t=a._self._c||e;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("div",{staticClass:"language-bibtex line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("@inproceedings{ServerDrivenVideoStreamingDeep2020,\n  title = {Server-{{Driven Video Streaming}} for {{Deep Learning Inference}}},\n  booktitle = {Proceedings of the {{Annual}} Conference of the {{ACM Special Interest Group}} on {{Data Communication}} on the Applications, Technologies, Architectures, and Protocols for Computer Communication},\n  author = {Du, Kuntai and Pervaiz, Ahsan and Yuan, Xin and Chowdhery, Aakanksha and Zhang, Qizheng and Hoffmann, Henry and Jiang, Junchen},\n  date = {2020-07-30},\n  pages = {557--570},\n  publisher = {{ACM}},\n  location = {{Virtual Event USA}},\n  doi = {10.1145/3387514.3405887},\n  url = {https://dl.acm.org/doi/10.1145/3387514.3405887},\n  urldate = {2021-01-24},\n  eventtitle = {{{SIGCOMM}} '20: {{Annual}} Conference of the {{ACM Special Interest Group}} on {{Data Communication}} on the Applications, Technologies, Architectures, and Protocols for Computer Communication},\n  isbn = {978-1-4503-7955-7},\n  langid = {english}\n}\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br"),t("span",{staticClass:"line-number"},[a._v("9")]),t("br"),t("span",{staticClass:"line-number"},[a._v("10")]),t("br"),t("span",{staticClass:"line-number"},[a._v("11")]),t("br"),t("span",{staticClass:"line-number"},[a._v("12")]),t("br"),t("span",{staticClass:"line-number"},[a._v("13")]),t("br"),t("span",{staticClass:"line-number"},[a._v("14")]),t("br"),t("span",{staticClass:"line-number"},[a._v("15")]),t("br")])]),t("h2",{attrs:{id:"概述"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#概述"}},[a._v("#")]),a._v(" 概述")]),a._v(" "),t("h3",{attrs:{id:"应用场景"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#应用场景"}},[a._v("#")]),a._v(" 应用场景")]),a._v(" "),t("ul",[t("li",[a._v("摄像头向服务器传输视频")]),a._v(" "),t("li",[a._v("服务器DNN执行目标检测或语义分割")]),a._v(" "),t("li",[a._v("网速不足以支持高清视频")])]),a._v(" "),t("h3",{attrs:{id:"传统方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#传统方法"}},[a._v("#")]),a._v(" 传统方法")]),a._v(" "),t("ol",[t("li",[a._v("根据网速情况调整传输视频的清晰度")]),a._v(" "),t("li",[a._v("摄像头先判断图像的重点区域，只传输这些重点区域")])]),a._v(" "),t("h3",{attrs:{id:"本文方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#本文方法"}},[a._v("#")]),a._v(" 本文方法")]),a._v(" "),t("ul",[t("li",[a._v("摄像头先传低清帧")]),a._v(" "),t("li",[a._v("服务器端的DNN反馈重点区域给摄像头")]),a._v(" "),t("li",[a._v("摄像头再把重点区域发给服务器端进一步识别")])]),a._v(" "),t("h2",{attrs:{id:"introduction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[a._v("#")]),a._v(" Introduction")]),a._v(" "),t("h3",{attrs:{id:"视频流领域的关键设计问题：在何处进行优化决策"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#视频流领域的关键设计问题：在何处进行优化决策"}},[a._v("#")]),a._v(" 视频流领域的关键设计问题：在何处进行优化决策")]),a._v(" "),t("h4",{attrs:{id:"source-driven：在视频流源端进行优化决策"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#source-driven：在视频流源端进行优化决策"}},[a._v("#")]),a._v(" Source-Driven：在视频流源端进行优化决策")]),a._v(" "),t("ul",[t("li",[a._v("常见于视频播放应用：Youtube、Netflix等")]),a._v(" "),t("li",[a._v("常见的框架：DASH(Dynamic Adaptive Streaming over HTTP)")])]),a._v(" "),t("p",[a._v("Source-Driven适用的前提：")]),a._v(" "),t("ol",[t("li",[a._v("application-level quality可以被Source端知晓")]),a._v(" "),t("li",[a._v("Destination端的user experience无法实时测量")])]),a._v(" "),t("h4",{attrs:{id:"实时目标识别领域source-driven为什么不好？"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#实时目标识别领域source-driven为什么不好？"}},[a._v("#")]),a._v(" 实时目标识别领域Source-Driven为什么不好？")]),a._v(" "),t("p",[a._v("在实时目标识别领域，Source-Driven意味着摄像头需要承担优化任务：决定视频流质量、决定传帧的哪些部分等")]),a._v(" "),t("p",[a._v("Source-Driven前提不满足：")]),a._v(" "),t("ol",[t("li",[a._v("Source端（摄像头）无法估计application-level quality（服务器上的识别准确率）")]),a._v(" "),t("li",[a._v("Destination端（服务器）的user experience（DNN输出的置信度）可以实时测量")])]),a._v(" "),t("h3",{attrs:{id:"本文如何解决这个问题：dds-dnn-driven-streaming"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#本文如何解决这个问题：dds-dnn-driven-streaming"}},[a._v("#")]),a._v(" 本文如何解决这个问题：DDS(DNN-Driven Streaming)")]),a._v(" "),t("p",[a._v("根据服务端的DNN输出进行优化决策")]),a._v(" "),t("ul",[t("li",[a._v("摄像头先传低清帧")]),a._v(" "),t("li",[a._v("服务器端的DNN反馈低清晰度上的识别结果给摄像头")]),a._v(" "),t("li",[a._v("摄像头根据反馈的识别结果把重点区域发给服务器端进一步识别")])]),a._v(" "),t("h3",{attrs:{id:"挑战：dnn反馈哪些识别结果"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#挑战：dnn反馈哪些识别结果"}},[a._v("#")]),a._v(" 挑战：DNN反馈哪些识别结果")]),a._v(" "),t("p",[a._v("本文主要关注于三个领域：")]),a._v(" "),t("ul",[t("li",[a._v("目标检测")]),a._v(" "),t("li",[a._v("语义分割")]),a._v(" "),t("li",[a._v("面部识别")])]),a._v(" "),t("p",[a._v("观察到的现象：低质图虽然不能给推断带来高正确率，但可以很好地找出疑似目标的位置")]),a._v(" "),t("h3",{attrs:{id:"现有的类似解决方案"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#现有的类似解决方案"}},[a._v("#")]),a._v(" 现有的类似解决方案")]),a._v(" "),t("ol",[t("li",[a._v("由摄像头判断哪些位置重要\n"),t("ul",[t("li",[a._v("RPN(region-proposal networks)：一种专用于判断图像重点区域位置的网络")]),a._v(" "),t("li",[a._v("缺陷：RPN只在目标较大时效果好；不适用于语义分割")])])]),a._v(" "),t("li",[a._v("注意力机制\n"),t("ul",[t("li",[a._v("与本文非常类似，都是关注于重点区域")])])])]),a._v(" "),t("h2",{attrs:{id:"motivation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#motivation"}},[a._v("#")]),a._v(" Motivation")]),a._v(" "),t("h3",{attrs:{id:"强终端vs弱终端-强服务器：省钱！"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#强终端vs弱终端-强服务器：省钱！"}},[a._v("#")]),a._v(" 强终端vs弱终端+强服务器：省钱！")]),a._v(" "),t("p",[a._v("弱终端+强服务器：NVIDIA Tesla T4 GPU (with a throughput of running ResNet50 at 5,700FPS) costs $23 × 60(cameras)+$2000(GPU)= $3.4K")]),a._v(" "),t("p",[a._v("强终端：60 NVIDIA Jetson TX2 cameras (each running ResNet50 at 89FPS)\ncosts about $400×60 = $24K")]),a._v(" "),t("h3",{attrs:{id:"性能参数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#性能参数"}},[a._v("#")]),a._v(" 性能参数")]),a._v(" "),t("ul",[t("li",[a._v("Accuracy：本文的识别结果和DNN直接处理高清全图的结果之间的相似性")]),a._v(" "),t("li",[a._v("Bandwidth usage：数据传输量/时间")]),a._v(" "),t("li",[a._v("Average response delay (freshness)：处理时间/识别目标数量（或语义分割中的像素数量）")])]),a._v(" "),t("h3",{attrs:{id:"可以进行设计的部分"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#可以进行设计的部分"}},[a._v("#")]),a._v(" 可以进行设计的部分")]),a._v(" "),t("ul",[t("li",[a._v("Leveraging camera-side compute power：使用相机提前去除无用部分\n"),t("ul",[t("li",[a._v("缺点：由于性能限制，精度很低")])])]),a._v(" "),t("li",[a._v("Model distillation：裁剪模型\n"),t("ul",[t("li",[a._v("缺点：This approach is efficient only in training smaller DNNs that work well on less expensive hardware.（没懂，什么意思）")])])]),a._v(" "),t("li",[a._v("Video codec optimization：使用基于内容的编码方式")]),a._v(" "),t("li",[a._v("Temporal configuration adaptation：基于视频内容调整帧码率\n"),t("ul",[t("li",[a._v("一帧只能一种编码，无法适应帧内部的内容分布不均")])])]),a._v(" "),t("li",[a._v("Spatial quality adaptation：根据帧内部的内容分布调整不同区域的码率")])]),a._v(" "),t("h3",{attrs:{id:"潜在的优化空间"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#潜在的优化空间"}},[a._v("#")]),a._v(" 潜在的优化空间")]),a._v(" "),t("ul",[t("li",[a._v("给人看的视频流要求每个地方都清晰，要有背景，还要流畅回放，给机器看的视频不需要这些")]),a._v(" "),t("li",[a._v("一个视频里，有50%~80%的视频帧中目标像素少于20%")])]),a._v(" "),t("p",[a._v("因此，根据帧内内容分布裁剪背景是一个很好的选择。")]),a._v(" "),t("h3",{attrs:{id:"现有方法的局限性"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#现有方法的局限性"}},[a._v("#")]),a._v(" 现有方法的局限性")]),a._v(" "),t("ul",[t("li",[a._v("相机侧进行切分：算力不足以支持精确的算法")]),a._v(" "),t("li",[a._v("服务侧全部包办：消耗大量带宽，一点点的性能提升就要多耗费巨大的带宽资源")])]),a._v(" "),t("h2",{attrs:{id:"系统设计"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#系统设计"}},[a._v("#")]),a._v(" 系统设计")]),a._v(" "),t("h3",{attrs:{id:"server-driven和传统source-driven对比👇"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#server-driven和传统source-driven对比👇"}},[a._v("#")]),a._v(" Server-Driven和传统Source-Driven对比👇")]),a._v(" "),t("p",[t("img",{attrs:{src:"i/DDS.png",alt:""}})]),a._v(" "),t("h3",{attrs:{id:"哪些区域需要高清？👇"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#哪些区域需要高清？👇"}},[a._v("#")]),a._v(" 哪些区域需要高清？👇")]),a._v(" "),t("p",[t("img",{attrs:{src:"i/DDS1.png",alt:""}})]),a._v(" "),t("ul",[t("li",[a._v("目标检测应用（结果为很多框，每个框对每个标签都有一个分数）\n"),t("ul",[t("li",[a._v("哪些区域不用高清feedback region：分数很高的框、很大的框")]),a._v(" "),t("li",[a._v("哪些区域需要高清feedback region：除上述框之外的所有识别出目标的框都要高清")])])]),a._v(" "),t("li",[a._v("语义分割应用（结果为每个像素对每个标签都有一个分数）\n"),t("ul",[t("li",[a._v("哪些区域需要高清feedback region：分数最高的标签和分数第二高的标签分数差小的像素\n"),t("ul",[t("li",[a._v("用框框住尽可能多的高分数差像素")])])])])])]),a._v(" "),t("h3",{attrs:{id:"如何适应变化的带宽？"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何适应变化的带宽？"}},[a._v("#")]),a._v(" 如何适应变化的带宽？")]),a._v(" "),t("ul",[t("li",[a._v("依据：上一帧传输时的所用带宽、当前待传输帧的可用带宽（通过卡尔曼滤波估计）")]),a._v(" "),t("li",[a._v("调节：高清视频流清晰度、低清视频流清晰度")])]),a._v(" "),t("h3",{attrs:{id:"如何传帧局部？"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#如何传帧局部？"}},[a._v("#")]),a._v(" 如何传帧局部？")]),a._v(" "),t("ul",[t("li",[a._v("将feedback region之外的区域置0，让视频编码器去压缩")])])])}),[],!1,null,null,null);e.default=i.exports}}]);